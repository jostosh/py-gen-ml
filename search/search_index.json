{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"py-gen-ml <p>A library for generating machine learning code from protobuf schemas.</p>"},{"location":"#project-introduction","title":"\ud83c\udf1f Project Introduction","text":"<p><code>py-gen-ml</code> simplifies the configuration and management of machine learning projects. It leverages Protocol Buffers (protobufs) to provide a robust, strongly typed, and extensible way to define and manipulate configuration schemas for machine learning projects. The protobuf schemas provide a single source of truth from which many things \u2728 are generated automatically \u2728.</p>"},{"location":"#brief-overview","title":"\u2728 Brief Overview","text":"<p>A real quick overview of what you can do with <code>py-gen-ml</code>:</p> <ul> <li> Define protos <pre><code>// Multi-layer perceptron configuration\nmessage MLPQuickstart {\n    option (pgml.cli).enable = true;\n    // Number of layers\n    int64 num_layers = 1;\n    // Number of units\n    int64 num_units = 2;\n    // Activation function\n    string activation = 3;\n}\n</code></pre> </li> </ul> <ul> <li> Generated Base Model <pre><code>class MLPQuickstart(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: int\n    \"\"\"Number of layers\"\"\"\n\n    num_units: int\n    \"\"\"Number of units\"\"\"\n\n    activation: str\n    \"\"\"Activation function\"\"\"\n</code></pre> </li> </ul> <ul> <li> Generated Patch Config <pre><code>class MLPQuickstartPatch(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: typing.Optional[int] = None\n    \"\"\"Number of layers\"\"\"\n\n    num_units: typing.Optional[int] = None\n    \"\"\"Number of units\"\"\"\n\n    activation: typing.Optional[str] = None\n    \"\"\"Activation function\"\"\"\n</code></pre> </li> </ul> <ul> <li> Generated Sweep Config <pre><code>class MLPQuickstartSweep(pgml.Sweeper[patch.MLPQuickstartPatch]):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: typing.Optional[pgml.IntSweep] = None\n    \"\"\"Number of layers\"\"\"\n\n    num_units: typing.Optional[pgml.IntSweep] = None\n    \"\"\"Number of units\"\"\"\n\n    activation: typing.Optional[pgml.StrSweep] = None\n    \"\"\"Activation function\"\"\"\n</code></pre> </li> </ul> <ul> <li> Generated CLI Parser <pre><code>class MLPQuickstartArgs(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of layers. Maps to 'num_layers'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"num_layers\"),\n    ]\n    \"\"\"Number of layers\"\"\"\n\n    num_units: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of units. Maps to 'num_units'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"num_units\"),\n    ]\n    \"\"\"Number of units\"\"\"\n# Remaining code...\n</code></pre> </li> </ul> <ul> <li> Generated Entrypoint <pre><code>@pgml.pgml_cmd(app=app)\ndef main(\n    config_paths: typing.List[str] = typer.Option(..., help=\"Paths to config files\"),\n    sweep_paths: typing.List[str] = typer.Option(\n        default_factory=list,\n        help=\"Paths to sweep files\"\n    ),\n    cli_args: cli_args.MLPQuickstartArgs = typer.Option(...),\n) -&gt; None:\n    mlp_quickstart = base.MLPQuickstart.from_yaml_files(config_paths)\n    mlp_quickstart = mlp_quickstart.apply_cli_args(cli_args)\n    if len(sweep_paths) == 0:\n        run_trial(mlp_quickstart)\n        return\n    # Remaining code....\n</code></pre> </li> </ul> <ul> <li> Flexible YAML Config <pre><code># base.yaml\nlayers:\n- num_units: 100\n  activation: \"#/_defs/activation\"\n- num_units: 50\n  activation: \"#/_defs/activation\"\noptimizer:\n  type: adamw\n  learning_rate: 1e-4\n  schedule: '!cosine_schedule.yaml'\n_defs_:\n  activation: relu\n</code></pre> <pre><code># cosine_schedule.yaml\nmin_lr: 1e-5\nmax_lr: 1e-3\n</code></pre> </li> </ul> <ul> <li> Flexible YAML sweeps <pre><code>layers:\n- num_units:  # Sample from a list\n  - 100\n  - 50\n  activation: \"#/_defs/activation\"\n- num_units:  # Sample from a range\n    low: 10\n    high: 100\n    step: 10\n  activation: \"#/_defs/activation\"\n_defs_:\n  activation: relu\n</code></pre> </li> </ul> <ul> <li> Instant YAML validation w/ JSON schemas <p></p> </li> </ul>"},{"location":"#key-features","title":"\ud83d\udd11 Key Features","text":"<p>\ud83d\udccc Single Source of Truth:</p> <ul> <li>The Protobuf schema provides a centralized definition for your configurations.</li> </ul> <p>\ud83d\udd27 Flexible Configuration Management:</p> <ul> <li>Minimal Change Amplification: Automatically generated code reduces cascading manual changes when modifying configurations.</li> <li>Flexible Patching: Easily modify base configurations with patches for quick experimentation.</li> <li>Flexible YAML: Use human-readable YAML with support for advanced references within and across files.</li> <li>Hyperparameter Sweeps: Effortlessly define and manage hyperparameter tuning.</li> <li>CLI Argument Parsing: Automatically generate command-line interfaces from your configuration schemas.</li> </ul> <p>\u2705 Validation and Type Safety:</p> <ul> <li>JSON Schema Generation: Easily validate your YAML content as you type.</li> <li>Strong Typing: The generated code comes with strong typing that will help you, your IDE, the type checker and your team to better understand the codebase and to build more robust ML code.</li> </ul>"},{"location":"#getting-started","title":"\ud83d\udea6 Getting Started","text":"<p>To start using py-gen-ml, you can install it via pip:</p> <pre><code>pip install py-gen-ml\n</code></pre> <p>For a quick example of how to use py-gen-ml in your project, check out our Quick Start Guide.</p>"},{"location":"#motivation","title":"\ud83d\udca1 Motivation","text":"<p>Machine learning projects often involve complex configurations with many interdependent parameters. Changing one config (e.g., the dataset) might require adjusting several other parameters for optimal performance. Traditional approaches to organizing configs can become unwieldy and tightly coupled with code, making changes difficult.</p> <p><code>py-gen-ml</code> addresses these challenges by:</p> <ol> <li>\ud83d\udcca Providing a single, strongly-typed schema definition for configurations.</li> <li>\ud83d\udd04 Generating code to manage configuration changes automatically.</li> <li>\ud83d\udcdd Offering flexible YAML configurations with advanced referencing and variable support.</li> <li>\ud83d\udee0\ufe0f Generating JSON schemas for real-time YAML validation.</li> <li>\ud83d\udd0c Seamlessly integrating into your workflow with multiple experiment running options:<ul> <li>Single experiments with specific config values</li> <li>Base config patching</li> <li>Parameter sweeps via JSON schema validated YAML files</li> <li>Quick value overrides via a generated CLI parser</li> <li>Arbitrary combinations of the above options</li> </ul> </li> </ol> <p>This approach results in more robust ML code, leveraging strong typing and IDE support while avoiding the burden of change amplification in complex configuration structures.</p>"},{"location":"#when-to-use-py-gen-ml","title":"\ud83c\udfaf When to use <code>py-gen-ml</code>","text":"<p>Consider using <code>py-gen-ml</code> when you need to:</p> <ul> <li>\ud83d\udcc8 Manage complex ML projects more efficiently</li> <li>\ud83d\udd2c Streamline experiment running and hyperparameter tuning</li> <li>\ud83d\udee1\ufe0f Reduce the impact of configuration changes on your workflow</li> <li>\ud83d\udcbb Leverage type safety and IDE support in your ML workflows</li> </ul>"},{"location":"#where-to-go-from-here","title":"\ud83d\udcda Where to go from here","text":"<ul> <li>Quickstart: A quick intro to the most important concepts.</li> <li>Command Line Interface: How to use the generated CLI parser.</li> <li>Parameter Sweeps: How to run parameter sweeps.</li> <li>Generated factories: How to generate factories to instantiate your classes.</li> <li>Cifar 10 example project: A more elaborate example of a machine learning project using <code>py-gen-ml</code>.</li> </ul>"},{"location":"py-gen-ml-command/","title":"py-gen-ml script","text":""},{"location":"py-gen-ml-command/#py-gen-ml","title":"<code>py-gen-ml</code>","text":"<p>Generate Pydantic models from a protobuf definition.</p> <p>This command is the core of the <code>py-gen-ml</code> toolbox. It is used to generate Pydantic models from a protobuf definition. By default, it generates models in the <code>src/pgml_out</code> directory. If your proto is called <code>example.proto</code>, it generates the following files:</p> <ul> <li><code>src/pgml_out/example_base.py</code> for a base model that follows the protobuf definition</li> <li><code>src/pgml_out/example_sweep.py</code> for a sweep model that can be used to sweep the base model</li> <li><code>src/pgml_out/example_patch.py</code> for a patch model that can be used to patch the base model</li> </ul> <p>If you've set the CLI option on a message called <code>Foo</code>, it will also generate</p> <ul> <li><code>src/pgml_out/example_cli_args.py</code> for CLI argument models</li> <li><code>src/pgml_out/foo_entrypoint.py</code> for an entrypoint that combines the base config, sweep, and CLI arguments.</li> </ul> <p>Other than that, it will generate JSON schemas in the <code>configs</code> directory.</p> <p>The structure of the schemas is as follows:</p> <ul> <li><code>configs/base/schemas/&lt;message_name&gt;.json</code> </li> <li><code>configs/patch/schemas/&lt;message_name&gt;.json</code></li> <li><code>configs/sweep/schemas/&lt;message_name&gt;.json</code></li> </ul> <p>Usage:</p> <pre><code>$ py-gen-ml [OPTIONS] PROTO_FILE...\n</code></pre> <p>Arguments:</p> <ul> <li><code>PROTO_FILE...</code>: Path to the protobuf file.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--proto-root TEXT</code>: Path to the root of the protobuf files which will be passed to theprotoc command as an include path. If not specified, the script will try to infer it from the proto_file arguments by adding the parent of the proto_file arguments.</li> <li><code>--code-dir TEXT</code>: Path to the generated code directory.  [default: src/pgml_out]</li> <li><code>--source-root TEXT</code>: Path to the root of the source code.  [default: src]</li> <li><code>--configs-dir TEXT</code>: Path to the base directory for configs.  [default: configs]</li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#quick-start-guide","title":"\ud83d\ude80 Quick Start Guide","text":""},{"location":"quickstart/#introduction","title":"\ud83c\udf1f Introduction","text":"<p><code>py-gen-ml</code> leverages protobufs to define the schema for your configuration. <code>py-gen-ml</code> uses the language agnostic schema to generate code and JSON schemas from the protobuf definitions, creating a robust and versatile configuration system for machine learning projects.</p> <p>Note</p> <p>While <code>py-gen-ml</code> currently doesn't fully utilize the language-neutral or platform-neutral features of protobuf, these capabilities are available for future expansion. If you're new to protobufs, you can learn more about them here.</p>"},{"location":"quickstart/#defining-your-protobuf","title":"\ud83d\udcdd Defining Your Protobuf","text":"<p>To create a protobuf schema, you'll need to write a <code>.proto</code> file. This file contains the definition of the data structure you want to use in your configuration. The protobuf counterpart of a data object is called a <code>message</code>. Most generated files we'll see later on will contain one class per message in the protobuf file.</p> <p>Here's a simple example of a protobuf definition:</p> <pre><code>// quickstart_a.proto\nsyntax = \"proto3\";\n\npackage example;\n\n// Multi-layer perceptron configuration\nmessage MLPQuickstart {\n    // Number of layers\n    int64 num_layers = 1;\n    // Number of units\n    int64 num_units = 2;\n    // Activation function\n    string activation = 3;\n}\n</code></pre>"},{"location":"quickstart/#generating-configuration-utilities","title":"\ud83d\udee0\ufe0f Generating Configuration Utilities","text":"<p>With your protobuf defined, you can now \u2728 generate \u2728 configuration objects using this command:</p> <pre><code>py-gen-ml quickstart_a.proto\n</code></pre> <p>By default, the generated code will be written to <code>src/pgml_out</code>. To customize this and explore other options, check out the py-gen-ml command documentation. The command will generate the following files:</p> <ul> <li><code>quickstart_a_base.py</code></li> <li><code>quickstart_a_patch.py</code></li> <li><code>quickstart_a_sweep.py</code></li> </ul> <p>Let's dive into the details of each file.</p>"},{"location":"quickstart/#generated-code","title":"\ud83e\udde9 Generated Code","text":""},{"location":"quickstart/#generated-base-model","title":"\ud83d\udcca Generated Base Model","text":"<p>One of the files generated is a Pydantic model for your main configuration.  </p><pre><code># Autogenerated code. DO NOT EDIT.\nimport py_gen_ml as pgml\n\n\nclass MLPQuickstart(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: int\n    \"\"\"Number of layers\"\"\"\n\n    num_units: int\n    \"\"\"Number of units\"\"\"\n\n    activation: str\n    \"\"\"Activation function\"\"\"\n</code></pre> <p>Use this file to load and validate configuration files written in YAML format. As you can see, it inherits from <code>pgml.YamlBaseModel</code> which is a convenience base class that provides methods for loading configurations from YAML files.</p> <p>For instance, the following YAML file will be validated according to the schema defined in <code>quickstart_a_base.py</code>:</p> <pre><code># example.yaml\nnum_layers: 2\nnum_units: 100\nactivation: relu\n</code></pre> <p>You can load the configuration like so:</p> <pre><code># example.py\nfrom pgml_out.quickstart_a_base import MLPQuickstart\n\nconfig = MLPQuickstart.from_yaml_file(\"example.yaml\")\n</code></pre>"},{"location":"quickstart/#generated-patch","title":"\ud83d\udd27 Generated Patch","text":"<pre><code># Autogenerated code. DO NOT EDIT.\nimport typing\nimport py_gen_ml as pgml\n\n\nclass MLPQuickstartPatch(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: typing.Optional[int] = None\n    \"\"\"Number of layers\"\"\"\n\n    num_units: typing.Optional[int] = None\n    \"\"\"Number of units\"\"\"\n\n    activation: typing.Optional[str] = None\n    \"\"\"Activation function\"\"\"\n</code></pre> <p>This file defines a Pydantic model for your patch configuration. All fields are optional. This allows you to express experiments in terms of changes with respect to a base configuration. </p> <p>Consequently:</p> <ul> <li>Changes are small and additive</li> <li>You can easily compose multiple patches together</li> </ul> <p>You can load a base configuration and apply patches using the <code>.from_yaml_files</code> method. This method is automatically inherited from <code>pgml.YamlBaseModel</code>:</p> <pre><code># example.py\nfrom pgml_out.quickstart_a_base import MLPQuickstart\nfrom pgml_out.quickstart_a_patch import MLPQuickstartPatch\n\nconfig_with_patches = MLPQuickstart.from_yaml_files([\"example.yaml\", \"example_patch.yaml\"])\n</code></pre>"},{"location":"quickstart/#generated-sweep-configuration","title":"\ud83d\udd0d Generated Sweep Configuration","text":"<p>Upon running the command, you'll also get a <code>quickstart_a_sweep.py</code> file:</p> <pre><code>import typing\n\nimport py_gen_ml as pgml\n\nfrom . import quickstart_a_patch as patch\nfrom . import quickstart_a_base as base\n\n\nclass MLPQuickstartSweep(pgml.Sweeper[patch.MLPQuickstartPatch]):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: typing.Optional[pgml.IntSweep] = None\n    \"\"\"Number of layers\"\"\"\n\n    num_units: typing.Optional[pgml.IntSweep] = None\n    \"\"\"Number of units\"\"\"\n\n    activation: typing.Optional[pgml.StrSweep] = None\n    \"\"\"Activation function\"\"\"\n\n\n\nMLPQuickstartSweepField = typing.Union[\n    MLPQuickstartSweep,\n    pgml.NestedChoice[MLPQuickstartSweep, patch.MLPQuickstartPatch],  # type: ignore\n]\n</code></pre> <p>This file defines a <code>pgml.Sweeper</code> for your configuration, enabling you to sweep over the values of your configuration. <code>py_gen_ml</code> comes with tooling to traverse the config and construct a search space for your trials. Currently, it supports Optuna but we'll add more frameworks in the future.</p> <p>Here's an example YAML file that will be validated according to the schema in <code>quickstart_a_sweep.py</code>:</p> <pre><code># example_sweep.yaml\nnum_layers:\n  low: 1\n  high: 5\n</code></pre> <p>To run a hyperparameter sweep, you can use the OptunaSampler:</p> <pre><code># example.py\nfrom pgml_out.quickstart_base import MLP\nfrom pgml_out.quickstart_sweep import MLPSweep\n\ndef train_model(config: MLP) -&gt; float:\n    \"\"\"Train a model and return the accuracy\"\"\"\n\nif __name__ == \"__main__\":\n    config = MLP.from_yaml_file(\"example.yaml\")\n    sweep = MLPSweep.from_yaml_file(\"example_sweep.yaml\")\n\n    def objective(trial: optuna.Trial) -&gt; float:\n        sampler = pgml.OptunaSampler(trial=trial)\n        patch = sampler.sample(sweep)\n        accuracy = train_model(config.merge(patch))\n        return accuracy\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n</code></pre>"},{"location":"quickstart/#generating-a-command-line-interface","title":"\ud83e\ude84 Generating a Command Line Interface","text":"<p>To generate a command line interface, you'll need to add the following option to your protobuf:</p> <pre><code>option (pgml.cli).enable = true;\n</code></pre> <p>Like so:</p> <pre><code>// quickstart_b.proto\nsyntax = \"proto3\";\n\npackage example;\n\nimport \"py_gen_ml/extensions.proto\";\n\n// Multi-layer perceptron configuration\nmessage MLPQuickstart {\n    option (pgml.cli).enable = true;\n    // Number of layers\n    int64 num_layers = 1;\n    // Number of units\n    int64 num_units = 2;\n    // Activation function\n    string activation = 3;\n}\n</code></pre> <p>When running <code>py-gen-ml</code>, you'll now get a <code>quickstart_b_cli_args.py</code> file:</p> <pre><code>py-gen-ml quickstart_b.proto\n</code></pre>"},{"location":"quickstart/#generated-cli","title":"\ud83d\udcbb Generated CLI","text":"<pre><code># Autogenerated code. DO NOT EDIT.\nimport py_gen_ml as pgml\nimport typing\n\nimport pydantic\nimport typer\n\nfrom . import quickstart_b_base as base\n\n\nclass MLPQuickstartArgs(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of layers. Maps to 'num_layers'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"num_layers\"),\n    ]\n    \"\"\"Number of layers\"\"\"\n\n    num_units: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of units. Maps to 'num_units'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"num_units\"),\n    ]\n    \"\"\"Number of units\"\"\"\n\n    activation: typing.Annotated[\n        typing.Optional[str],\n        typer.Option(help=\"Activation function. Maps to 'activation'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"activation\"),\n    ]\n    \"\"\"Activation function\"\"\"\n</code></pre> <p>This file defines a Pydantic model for your command line arguments. We've chosen to use typer to handle command line arguments, and we've added a convenience function to simplify the use of this class.</p> <p>The easiest way to use the CLI is to copy the generated entrypoint script. The entrypoint name is the snake case version of the name of the message with the <code>pgml.cli</code> option with <code>_entrypoint.py</code> appended.</p>"},{"location":"quickstart/#generated-entrypoint","title":"\ud83d\ude80 Generated Entrypoint","text":"<pre><code>import pgml_out.quickstart_b_base as base\nimport pgml_out.quickstart_b_sweep as sweep\nimport pgml_out.quickstart_b_cli_args as cli_args\nimport typer\nimport py_gen_ml as pgml\nimport optuna\nimport typing\n\napp = typer.Typer(pretty_exceptions_enable=False)\n\ndef run_trial(\n    mlp_quickstart: base.MLPQuickstart,\n    trial: typing.Optional[optuna.Trial] = None\n) -&gt; typing.Union[float, typing.Sequence[float]]:\n    \"\"\"\n    Run a trial with the given values for mlp_quickstart. The sampled\n    hyperparameters have already been added to the trial.\n    \"\"\"\n    # TODO: Implement this function\n    return 0.0\n\n@pgml.pgml_cmd(app=app)\ndef main(\n    config_paths: typing.List[str] = typer.Option(..., help=\"Paths to config files\"),\n    sweep_paths: typing.List[str] = typer.Option(\n        default_factory=list,\n        help=\"Paths to sweep files\"\n    ),\n    cli_args: cli_args.MLPQuickstartArgs = typer.Option(...),\n) -&gt; None:\n    mlp_quickstart = base.MLPQuickstart.from_yaml_files(config_paths)\n    mlp_quickstart = mlp_quickstart.apply_cli_args(cli_args)\n    if len(sweep_paths) == 0:\n        run_trial(mlp_quickstart)\n        return\n    mlp_quickstart_sweep = sweep.MLPQuickstartSweep.from_yaml_files(sweep_paths)\n\n    def objective(trial: optuna.Trial) -&gt; typing.Union[\n        float,\n        typing.Sequence[float]\n    ]:\n        optuna_sampler = pgml.OptunaSampler(trial)\n        mlp_quickstart_patch = optuna_sampler.sample(mlp_quickstart_sweep)\n        mlp_quickstart_patched = mlp_quickstart.merge(mlp_quickstart_patch)\n        objective_value = run_trial(mlp_quickstart_patched, trial)\n        return objective_value\n\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=100)\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>The magic happens in the <code>pgml.pgml_cmd</code> decorator. This decorator is used to wrap the <code>main</code> function and add the necessary arguments and options to the CLI.</p> <p>Now you can run your script with command line arguments and configuration files:</p> <pre><code>python mlp_quickstart_entrypoint.py --help\n</code></pre> <p>You can set parameters via both command line arguments and configuration files:</p> <pre><code>python mlp_quickstart_entrypoint.py --config-paths example.yaml --num-layers 3\n</code></pre> <p>With these tools at your disposal, you're now ready to create flexible and powerful configurations for your machine learning projects using <code>py-gen-ml</code>! If you're looking for a more complex example, check out the CIFAR 10 example project.</p>"},{"location":"example_projects/cifar10/","title":"CIFAR-10","text":""},{"location":"example_projects/cifar10/#introduction","title":"Introduction","text":"<p>Here we will walk through an example project for training a model to classify images from the CIFAR10 dataset. This combines most of the concepts we have seen in the guides so far. It offers a simple model, data and optimizer, and shows how to:</p> <ul> <li>Use a variety of special attributes to define the config schema</li> <li>Load a config from a yaml file</li> <li>Apply CLI arguments to the config</li> <li>Load a sweep file to sample from a set of configurations</li> </ul>"},{"location":"example_projects/cifar10/#schema","title":"Schema","text":"<p>The schema is defined in a proto file. This is the file that we will use to train the model. It looks like this:</p>"},{"location":"example_projects/cifar10/#the-model","title":"The model","text":"<p>The model definition is a simple convolutional neural network followed by a multi-layer perceptron. In proto definition, it looks like this:</p> <pre><code>// Model configuration\nmessage Model {\n    // Conv blocks\n    ConvNet conv_net = 1;\n    // MLP head\n    MLP head = 2;\n}\n\n// Convolutional neural network configuration\nmessage ConvNet {\n    // Conv layer configuration\n    ConvBlock block = 1;\n    // Number of layers\n    uint32 num_layers = 2 [(pgml.default).uint32 = 2];\n}\n\n// Multi-layer perceptron configuration\nmessage MLP {\n    // Linear layer configuration\n    LinearBlock block = 1;\n    // Number of layers\n    uint32 num_layers = 2 [(pgml.default).uint32 = 2];\n}\n\n// Convolutional layer configuration\nmessage ConvBlock {\n    option (pgml.factory) = \"cifar10.modules.ConvBlock\";\n    // Number of output channels\n    uint32 out_channels = 1  [(pgml.default).uint32 = 128];\n    // Square kernel size\n    uint32 kernel_size = 2 [(pgml.default).uint32 = 3];\n    // Square pool size\n    uint32 pool_size = 3 [(pgml.default).uint32 = 2];\n    // Activation function\n    Activation activation = 4 [(pgml.default).enum = \"GELU\"];\n}\n\n// Linear layer configuration\nmessage LinearBlock {\n    option (pgml.factory) = \"cifar10.modules.LinearBlock\";\n    // Number of output features\n    uint32 out_features = 1 [(pgml.default).uint32 = 128];\n    // Activation function\n    Activation activation = 2 [(pgml.default).enum = \"GELU\"];\n}\n\n// Activation function\nenum Activation {\n    // GELU activation\n    GELU = 0;\n    // ReLU activation\n    RELU = 1;\n}\n</code></pre> <p>Concepts that we've used here are:</p> <ul> <li>Nesting: we define a <code>Model</code> message that contains two nested messages: <code>ConvNet</code> and <code>MLP</code> which in turn contain blocks of linear and convolutional layers.</li> <li><code>pgml.factory</code>: This is a special attribute that tells <code>py-gen-ml</code> that with these values we can build an instance of the given class.</li> <li><code>pgml.default</code>: This is a special attribute that tells <code>py-gen-ml</code> to use a default value if the field is not set.</li> <li>Enums: we define an enum for the activation function.</li> </ul>"},{"location":"example_projects/cifar10/#the-data","title":"The data","text":"<p>We define a <code>Data</code> message that contains the batch size and the number of epochs to train. In proto definition, it looks like this:</p> <pre><code>// Data configuration\nmessage Data {\n    // Batch size for a single GPU\n    uint32 batch_size = 1 [(pgml.default).uint32 = 32];\n    // Number of epochs to train\n    uint32 num_epochs = 2 [(pgml.default).uint32 = 10];\n}\n</code></pre>"},{"location":"example_projects/cifar10/#the-optimizer","title":"The optimizer","text":"<p>We define an <code>Optimizer</code> message that contains the learning rate and the decay rate. In proto definition, it looks like this:</p> <pre><code>// Optimizer configuration\nmessage Optimizer {\n    // Learning rate\n    float learning_rate = 1 [(pgml.default).float = 1e-4];\n    // Decay rate\n    float beta1 = 2 [(pgml.default).float = 0.99];\n}\n</code></pre>"},{"location":"example_projects/cifar10/#the-project","title":"The project","text":"<p>We define a <code>Project</code> message that contains the model, the optimizer, and the data. In proto definition, it looks like this:</p> <p></p><pre><code>syntax = \"proto3\";\n\npackage cifar10;\n\nimport \"py_gen_ml/extensions.proto\";\n\n\n// Global configuration\nmessage Project {\n    option (pgml.cli) = {\n        enable: true\n        arg: {name: \"conv_activation\", path: \"net.conv_net.block.activation\"}\n        arg: {name: \"head_activation\", path: \"net.head.block.activation\"}\n        arg: {name: \"num_conv_layers\", path: \"net.conv_net.num_layers\"}\n        arg: {name: \"num_mlp_layers\", path: \"net.head.num_layers\"}\n    };\n    // Model configuration\n    Model net = 1;\n    // Optimizer configuration\n    Optimizer optimizer = 2;\n    // Data configuration\n    Data data = 3;\n}\n</code></pre> As you can see, there are a couple of arg references to ensure we can propagate values via the command line for fields that have the same name and that are deeply nested."},{"location":"example_projects/cifar10/#the-entrypoint","title":"The entrypoint","text":"<p>To launch the training, we create a function that can do a range of things:</p> <ol> <li>Load a yaml config file</li> <li>Load multiple yaml config files and merge them</li> <li>Load a yaml sweep file to generate patches and apply them to the config using Optuna</li> <li>Apply CLI arguments to the config</li> <li>Whatever the input is, train the model and return the accuracy</li> </ol> <p>Even though this sounds like a lot, with <code>pg-gen-ml</code> it is actually quite easy to do. The main function now becomes:</p> Entrypoint<pre><code>@pgml.pgml_cmd(app=app)\ndef main(\n    config_paths: List[str] = typer.Option(..., help='Paths to config files'),\n    sweep_paths: List[str] = typer.Option(default_factory=list, help='Paths to sweep files'),\n    cli_args: cli_args.ProjectArgs = typer.Option(...),\n) -&gt; None:\n    config = base.Project.from_yaml_files(config_paths)\n    config = config.apply_cli_args(cli_args)\n\n    if len(sweep_paths) == 0:\n        train_model(config, trial=None)\n        return\n\n    sweep_config = sweep.ProjectSweep.from_yaml_files(sweep_paths)\n\n    def objective(trial: optuna.Trial) -&gt; float:\n        sampler = pgml.OptunaSampler(trial=trial)\n        patch = sampler.sample(sweep_config)\n        accuracy = train_model(project=config.merge(patch), trial=trial)\n        return accuracy\n\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=100)\n</code></pre> <p>Let's break this down a bit more.</p> <ul> <li>At line 7, we load the project config from one or more yaml files. This is where files will be merged before they are passed to the Pydantic model validator.</li> <li>At line 8, we apply the CLI arguments to the config.</li> <li>At line 10, we check if there are any sweep files. If there are, we load them at line 14. If there are no sweep files, we simply train the model and return the accuracy.</li> <li>At line 16-20, we define an objective function for Optuna to use. This is the function that will be called to train the model and return the accuracy. Note that we are using the exact same <code>train_model</code> function that we used in line 10 earlier.</li> <li>At line 22 and 23, we create an Optuna study and optimize the objective function.</li> </ul>"},{"location":"example_projects/cifar10/#the-configuration","title":"The configuration","text":"<p>The configuration is defined in a yaml file. This is the file that we will use to train the model. It looks like this:</p> Config<pre><code># yaml-language-server: $schema=schemas/project.json\nnet:\n  conv_net:\n    block:\n      activation: GELU\n      kernel_size: 3\n      out_channels: 32\n      pool_size: 2\n  head:\n    block:\n      activation: GELU\n      out_features: 128\n    num_layers: 3\noptimizer:\n  beta1: 0.99\n  learning_rate: 0.0001\ndata:\n  batch_size: 32\n  num_epochs: 10\n</code></pre>"},{"location":"example_projects/cifar10/#launching-the-training","title":"Launching the training","text":"<p>To launch the training, we can use the following command:</p> <pre><code>python examples/cifar10/src/cifar10/train.py \\\n    --config-paths \\\n    configs/base/default.yaml\n</code></pre>"},{"location":"example_projects/cifar10/#showing-cli-arguments","title":"Showing CLI arguments","text":"<p>To show the CLI arguments, we can use the following command:</p> <pre><code>python examples/cifar10/src/cifar10/train.py --help\n</code></pre> <p>This will show the following: </p><pre><code> Usage: train.py [OPTIONS]\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --config-paths              TEXT         Path to config file [default: None] [required]                                             \u2502\n\u2502    --sweep-paths               TEXT         Type of config to use [default: &lt;class 'list'&gt;]                                            \u2502\n\u2502    --out-channels              INTEGER      Number of output channels. Maps to 'net.conv_net.block.out_channels' [default: None]       \u2502\n\u2502    --kernel-size               INTEGER      Square kernel size. Maps to 'net.conv_net.block.kernel_size' [default: None]               \u2502\n\u2502    --pool-size                 INTEGER      Square pool size. Maps to 'net.conv_net.block.pool_size' [default: None]                   \u2502\n\u2502    --out-features              INTEGER      Number of output features. Maps to 'net.head.block.out_features' [default: None]           \u2502\n\u2502    --batch-size                INTEGER      Batch size for a single GPU. Maps to 'data.batch_size' [default: None]                     \u2502\n\u2502    --num-epochs                INTEGER      Number of epochs to train. Maps to 'data.num_epochs' [default: None]                       \u2502\n\u2502    --learning-rate             FLOAT        Learning rate. Maps to 'optimizer.learning_rate' [default: None]                           \u2502\n\u2502    --beta1                     FLOAT        Decay rate. Maps to 'optimizer.beta1' [default: None]                                      \u2502\n\u2502    --conv-activation           [gelu|relu]  Activation function. Maps to 'net.conv_net.block.activation' [default: None]               \u2502\n\u2502    --head-activation           [gelu|relu]  Activation function. Maps to 'net.head.block.activation' [default: None]                   \u2502\n\u2502    --num-conv-layers           INTEGER      Number of layers. Maps to 'net.conv_net.num_layers' [default: None]                        \u2502\n\u2502    --num-mlp-layers            INTEGER      Number of layers. Maps to 'net.head.num_layers' [default: None]                            \u2502\n\u2502    --install-completion                     Install completion for the current shell.                                                  \u2502\n\u2502    --show-completion                        Show completion for the current shell, to copy it or customize the installation.           \u2502\n\u2502    --help                                   Show this message and exit.                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"example_projects/cifar10/#starting-a-sweep","title":"Starting a sweep","text":"<p>We define the following sweep configuration:</p> Sweep<pre><code># yaml-language-server: $schema=schemas/project.json\noptimizer:\n  beta1:\n    low: 0.9\n    high: 0.99\n  learning_rate:\n    log_low: 1e-5\n    log_high: 1e-3\n</code></pre> <p>This is a sweep over the learning rate and beta1 parameters.</p> <p>To run the sweep, we can use the following command:</p> <pre><code>python examples/cifar10/src/cifar10/train.py \\\n    --config-paths \\\n    configs/base/default.yaml \\\n    --sweep-paths \\\n    configs/sweep/lr_beta1.yaml\n</code></pre>"},{"location":"example_projects/cifar10/#remaining-code","title":"Remaining code","text":""},{"location":"example_projects/cifar10/#modules","title":"Modules","text":"<p>We have define the modules here </p>Modules<pre><code>from typing import Self\n\nimport pgml_out.config_base as base\nimport torch\n\nCLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n\ndef create_activation(activation: base.Activation) -&gt; torch.nn.Module:\n    if activation == base.Activation.GELU:\n        return torch.nn.GELU()\n    elif activation == base.Activation.RELU:\n        return torch.nn.ReLU()\n    else:\n        raise ValueError(f'Invalid activation function: {activation}')\n\n\nclass LinearBlock(torch.nn.Module):\n    \"\"\"Linear layer with activation configuration\"\"\"\n\n    def __init__(self, out_features: int, activation: base.Activation) -&gt; None:\n        super().__init__()\n        self.linear = torch.nn.LazyLinear(out_features=out_features)\n        self.activation = create_activation(activation)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.activation(self.linear(x))\n\n\nclass ConvBlock(torch.nn.Module):\n    \"\"\"Convolutional layer with activation configuration\"\"\"\n\n    def __init__(self, out_channels: int, kernel_size: int, pool_size: int, activation: base.Activation) -&gt; None:\n        super().__init__()\n        self.conv = torch.nn.LazyConv2d(out_channels=out_channels, kernel_size=kernel_size)\n        self.pool = torch.nn.MaxPool2d(kernel_size=pool_size)\n        self.activation = create_activation(activation)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.activation(self.pool(self.conv(x)))\n\n\nclass ConvNet(torch.nn.Module):\n\n    def __init__(self, block: base.ConvBlock, num_layers: int) -&gt; None:\n        super().__init__()\n        self.layers = [block.build() for _ in range(num_layers)]\n        self.net = torch.nn.Sequential(*self.layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.net(x)\n\n\nclass MLP(torch.nn.Module):\n\n    def __init__(self, block: base.LinearBlock, num_layers: int) -&gt; None:\n        super().__init__()\n        self.layers = [block.build() for _ in range(num_layers)]\n        self.net = torch.nn.Sequential(*self.layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.net(x)\n\n\nclass Model(torch.nn.Module):\n\n    def __init__(self, conv_net: ConvNet, head: MLP) -&gt; None:\n        super().__init__()\n        self.conv_net = conv_net\n        self.head = head\n        self.class_logits = torch.nn.LazyLinear(out_features=len(CLASSES),)\n        self.flatten = torch.nn.Flatten()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        x = self.conv_net(x)\n        x = self.head(x)\n        x = self.flatten(x)\n        x = self.class_logits(x)\n        return x\n\n    @classmethod\n    def from_config(cls, config: base.Model) -&gt; Self:\n        conv_net = ConvNet(\n            block=config.conv_net.block,\n            num_layers=config.conv_net.num_layers,\n        )\n        head = MLP(\n            block=config.head.block,\n            num_layers=config.head.num_layers,\n        )\n        return cls(conv_net, head)\n</code></pre>"},{"location":"example_projects/cifar10/#data","title":"Data","text":"<p>We have defined the data module here:</p> Data<pre><code>import os\nfrom typing import Any\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\ndef get_transform() -&gt; transforms.Compose:\n    return transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ],)\n\n\ndef get_data_loader(transform: transforms.Compose, batch_size: int, train: bool) -&gt; torch.utils.data.DataLoader[Any]:\n    trainset = torchvision.datasets.CIFAR10(\n        root=f\"{os.environ['HOME']}/data/torchvision\",\n        train=train,\n        download=train,\n        transform=transform,\n    )\n    return torch.utils.data.DataLoader[Any](\n        trainset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=4,\n        prefetch_factor=4,\n    )\n</code></pre>"},{"location":"example_projects/cifar10/#trainer","title":"Trainer","text":"<p>We have defined the trainer here:</p> Trainer<pre><code>class Trainer:\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        train_loader: torch.utils.data.DataLoader[Any],\n        test_loader: torch.utils.data.DataLoader[Any],\n        criterion: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        accuracy_metric_train: torchmetrics.classification.MulticlassAccuracy,\n        accuracy_metric_test: torchmetrics.classification.MulticlassAccuracy,\n        train_loss_metric: torchmetrics.MeanMetric,\n        test_loss_metric: torchmetrics.MeanMetric,\n        trial: Optional[optuna.Trial] = None,\n    ) -&gt; None:\n        self._train_loader = train_loader\n        self._test_loader = test_loader\n        self._model = model\n        self._criterion = criterion\n        self._optimizer = optimizer\n        self._accuracy_metric_train = accuracy_metric_train\n        self._accuracy_metric_test = accuracy_metric_test\n        self._train_loss_metric = train_loss_metric\n        self._test_loss_metric = test_loss_metric\n        self._trial = trial\n\n    def train(self, num_epochs: int) -&gt; float:\n        device = get_device()\n        step = 0\n        for epoch in tqdm.trange(num_epochs, position=0, desc='Epoch'):\n            batch_bar = tqdm.tqdm(self._train_loader, position=1, desc='Batch')\n            for inputs, labels in batch_bar:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                self._optimizer.zero_grad()\n                outputs = self._model(inputs)\n                loss = self._criterion(outputs, labels)\n                loss.backward()\n                self._optimizer.step()\n\n                self._train_loss_metric.update(loss)\n                self._accuracy_metric_train.update(outputs, labels)\n                step += 1\n\n                if step % 10 == 0:\n                    batch_bar.set_postfix(\n                        loss=self._train_loss_metric.compute().item(),\n                        accuracy=self._accuracy_metric_train.compute().item(),\n                    )\n                    self._train_loss_metric.reset()\n                    self._accuracy_metric_train.reset()\n\n            self._evaluate()\n            if self._trial is not None:\n                self._trial.report(self._accuracy_metric_test.compute().item(), epoch)\n        return self._accuracy_metric_test.compute().item()\n\n    @torch.inference_mode()\n    def _evaluate(self) -&gt; None:\n        self._model.eval()\n        self._accuracy_metric_test.reset()\n        self._test_loss_metric.reset()\n        for images, labels in tqdm.tqdm(self._test_loader, position=1, desc='Evaluating'):\n            images = images.to(get_device())\n            labels = labels.to(get_device())\n            outputs = self._model(images)\n            loss = self._criterion(outputs, labels)\n            self._test_loss_metric.update(loss)\n            self._accuracy_metric_test.update(outputs, labels)\n        print(f'Test accuracy: {self._accuracy_metric_test.compute().item()}')\n        print(f'Test loss: {self._test_loss_metric.compute().item()}')\n        self._model.train()\n</code></pre>"},{"location":"example_projects/cifar10/#train-function","title":"Train function","text":"<p>The train function that instantiates all the components and calls the trainer is defined here:</p> Train function<pre><code>def train_model(project: base.Project, trial: typing.Optional[optuna.Trial] = None) -&gt; float:\n    rich.print(project)\n\n    transform = get_transform()\n    train_loader = get_data_loader(transform=transform, batch_size=project.data.batch_size, train=True)\n    test_loader = get_data_loader(transform=transform, batch_size=project.data.batch_size, train=False)\n\n    device = get_device()\n    print(f'device {device}')\n\n    model = Model.from_config(config=project.net).to(device)\n    path = pathlib.Path(f\"{os.environ['HOME']}/gen_ml/logs/{uuid.uuid4()}\")\n    print(f'Storing logs at {path}')\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(params=model.parameters(), lr=project.optimizer.learning_rate)\n    accuracy_metric_train = get_accuracy_metric(num_classes=len(CLASSES))\n    accuracy_metric_test = get_accuracy_metric(num_classes=len(CLASSES))\n    train_loss_metric = torchmetrics.MeanMetric().to(device)\n    test_loss_metric = torchmetrics.MeanMetric().to(device)\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        test_loader=test_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        accuracy_metric_train=accuracy_metric_train,\n        accuracy_metric_test=accuracy_metric_test,\n        train_loss_metric=train_loss_metric,\n        test_loss_metric=test_loss_metric,\n        trial=trial,\n    )\n    accuracy = trainer.train(num_epochs=project.data.num_epochs)\n    return accuracy\n</code></pre>"},{"location":"guides/builders/","title":"Factories","text":""},{"location":"guides/builders/#factories","title":"\ud83c\udfed Factories","text":""},{"location":"guides/builders/#generated-factory-methods","title":"\ud83e\ude84 Generated Factory Methods","text":"<p>Occasionally, you have enough information to instantiate a class from a configuration object immediately. <code>py-gen-ml</code> allows you to generate factory methods in such cases. The factory methods unpack the message fields into keyword arguments and then instantiate an object.</p> <p>To specify a factory method for a message, you can use the <code>(pgml.factory)</code> option. For example:</p> <pre><code>// builder_demo.proto\nsyntax = \"proto3\";\n\npackage builder_demo;\n\n// Import the PGML extensions\nimport \"py_gen_ml/extensions.proto\";\n\n\n// Linear layer configuration\nmessage Linear {\n    option (pgml.factory) = \"torch.nn.Linear\";\n    // Number of input features\n    uint32 in_features = 1;\n    // Number of output features\n    uint32 out_features = 2;\n    // Bias\n    bool bias = 3;\n}\n\n// MLP configuration\nmessage MLP {\n    // Linear layers\n    repeated Linear layers = 1;\n}\n</code></pre> <p>The generated code will look like this:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport typing\nimport py_gen_ml as pgml\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    import torch.nn\n\n\nclass Linear(pgml.YamlBaseModel):\n    \"\"\"Linear layer configuration\"\"\"\n\n    in_features: int\n    \"\"\"Number of input features\"\"\"\n\n    out_features: int\n    \"\"\"Number of output features\"\"\"\n\n    bias: bool\n    \"\"\"Bias\"\"\"\n\n    def build(self) -&gt; \"torch.nn.Linear\":\n        import torch.nn\n\n        return torch.nn.Linear(\n            in_features=self.in_features,\n            out_features=self.out_features,\n            bias=self.bias,\n        )\n\n\nclass MLP(pgml.YamlBaseModel):\n    \"\"\"MLP configuration\"\"\"\n\n    layers: typing.List[Linear]\n    \"\"\"Linear layers\"\"\"\n</code></pre> <p>Notice the <code>build</code> method. This method is automatically generated for you. It unpacks the message fields into keyword arguments and then instantiates the class.</p> <p>In your experiment code, you can now call the <code>build</code> method to instantiate the class:</p> <pre><code>import torch\n\nfrom pgml_out.builder_demo_base import MLP\n\nif __name__ == \"__main__\":\n    mlp_config = MLP.from_yaml(\"configs/base/mlp.yaml\")\n\n    layers = [layer.build() for layer in mlp_config.layers]\n    mlp = torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"guides/builders/#using-custom-classes","title":"\ud83e\uddf1 Using custom classes","text":"<p>The builder extension can be used for any class, not just PyTorch classes. You can use it to instantiate any class that you have access to.</p> <p>For example, let's say you have a custom class that you want to instantiate. You can do this:</p> <pre><code># src/example_project/modules.py\nimport torch.nn\n\n\nclass LinearBlock(torch.nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias: bool = True, dropout: float = 0.0, activation: str = \"relu\"):\n        super().__init__()\n        self.linear = torch.nn.Linear(in_features, out_features, bias=bias)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.activation = torch.nn.ReLU() if activation == \"relu\" else torch.nn.GELU()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.dropout(self.activation(self.linear(x)))\n</code></pre> <p>And then define the following proto:</p> <pre><code>// builder_custom_class_demo.proto\nsyntax = \"proto3\";\n\npackage builder_custom_class_demo;\n\nimport \"py_gen_ml/extensions.proto\";\n\n\n// Linear block configuration\nmessage LinearBlock {\n    option (pgml.factory) = \"example_project.modules.LinearBlock\";\n    // Number of input features\n    uint32 in_features = 1;\n    // Number of output features\n    uint32 out_features = 2;\n    // Bias\n    bool bias = 3;\n    // Dropout probability\n    float dropout = 4;\n    // Activation function\n    string activation = 5;\n}\n\n// MLP configuration\nmessage MLP {\n    // Linear blocks\n    repeated LinearBlock layers = 1;\n}\n</code></pre> <p>The generated code will look like this:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport typing\nimport py_gen_ml as pgml\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    import example_project.modules\n\n\nclass LinearBlock(pgml.YamlBaseModel):\n    \"\"\"Linear block configuration\"\"\"\n\n    in_features: int\n    \"\"\"Number of input features\"\"\"\n\n    out_features: int\n    \"\"\"Number of output features\"\"\"\n\n    bias: bool\n    \"\"\"Bias\"\"\"\n\n    dropout: float\n    \"\"\"Dropout probability\"\"\"\n\n    activation: str\n    \"\"\"Activation function\"\"\"\n\n    def build(self) -&gt; \"example_project.modules.LinearBlock\":\n        import example_project.modules\n\n        return example_project.modules.LinearBlock(\n            in_features=self.in_features,\n            out_features=self.out_features,\n            bias=self.bias,\n            dropout=self.dropout,\n            activation=self.activation,\n        )\n\n\nclass MLP(pgml.YamlBaseModel):\n    \"\"\"MLP configuration\"\"\"\n\n    layers: typing.List[LinearBlock]\n    \"\"\"Linear blocks\"\"\"\n</code></pre>"},{"location":"guides/builders/#expanding-fields-as-varargs","title":"\ud83d\udca5 Expanding fields as varargs","text":"<p>You can also expand fields as varargs. This is useful if you have a list of arguments that you want to pass to the builder. For example, let's say you have a custom class that you want to instantiate. You can use the <code>(pgml.as_varargs)</code> option to expand the fields as varargs. For example:</p> <pre><code>// builder_varargs_demo.proto\nsyntax = \"proto3\";\n\npackage builder_varargs_demo;\n\nimport \"py_gen_ml/extensions.proto\";\n\n\n// Linear layer configuration\nmessage Linear {\n    option (pgml.factory) = \"torch.nn.Linear\";\n    // Number of input features\n    uint32 in_features = 1;\n    // Number of output features\n    uint32 out_features = 2;\n    // Bias\n    bool bias = 3;\n}\n\n// MLP configuration\nmessage MLP {\n    option (pgml.factory) = \"torch.nn.Sequential\";\n    // Linear layers\n    repeated Linear layers = 1 [(pgml.as_varargs) = true];\n}\n</code></pre> <p>The generated code will look like this:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport typing\nimport py_gen_ml as pgml\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    import torch.nn\n\n\nclass Linear(pgml.YamlBaseModel):\n    \"\"\"Linear layer configuration\"\"\"\n\n    in_features: int\n    \"\"\"Number of input features\"\"\"\n\n    out_features: int\n    \"\"\"Number of output features\"\"\"\n\n    bias: bool\n    \"\"\"Bias\"\"\"\n\n    def build(self) -&gt; \"torch.nn.Linear\":\n        import torch.nn\n\n        return torch.nn.Linear(\n            in_features=self.in_features,\n            out_features=self.out_features,\n            bias=self.bias,\n        )\n\n\nclass MLP(pgml.YamlBaseModel):\n    \"\"\"MLP configuration\"\"\"\n\n    layers: typing.List[Linear]\n    \"\"\"Linear layers\"\"\"\n\n    def build(self) -&gt; \"torch.nn.Sequential\":\n        import torch.nn\n\n        return torch.nn.Sequential(\n            *(elem.build() for elem in self.layers),\n        )\n</code></pre>"},{"location":"guides/builders/#nesting-factories","title":"\ud83d\udc23 Nesting factories","text":"<p>As you may have noticed, factories can also be nested. In the section on varargs, we see that the build method in <code>MLP</code> takes a varargs of <code>Linear</code> objects that are also instantiated with a factory. Nesting with factories can streamline instantiation of complex objects, but it also creates a tighter coupling between your schema and the objects that are created.</p> <p>Usually, it is best to use factories for objects that don't need other factories for their fields. In other words, you should nest factories sparingly.</p>"},{"location":"guides/cli_argument_parsing/","title":"CLI argument parsing","text":""},{"location":"guides/cli_argument_parsing/#cli-argument-parsing","title":"\ud83d\udda5\ufe0f CLI Argument Parsing","text":""},{"location":"guides/cli_argument_parsing/#implicit-argument-references","title":"\u2728 Implicit Argument References","text":"<p><code>py-gen-ml</code> generates a smart CLI argument parser using Pydantic base models. It shortens CLI argument names for deeply nested fields in your config when there's exactly one path to a field and the field name is unique.</p> <p>Example protobuf structure:</p> <pre><code>// cli_demo.proto\nsyntax = \"proto3\";\n\npackage cli_demo;\n\nimport \"py_gen_ml/extensions.proto\";\n\n// Global configuration\nmessage CLIDemo {\n    option (pgml.cli).enable = true;\n    // Dataset configuration\n    Data data = 1;\n    // Model configuration\n    Model model = 2;\n    // Training configuration\n    Training training = 3;\n}\n\n// Dataset configuration\nmessage Dataset {\n    // Path to the dataset\n    string path = 1;\n}\n\n// Data config\nmessage Data {\n    // Path to the dataset\n    Dataset dataset = 1;\n    // Number of workers for loading the dataset\n    uint32 num_workers = 2;\n}\n\n// Model configuration\nmessage Model {\n    // Number of layers\n    uint32 num_layers = 1;\n}\n\n// Training configuration\nmessage Training {\n    // Number of epochs\n    uint32 num_epochs = 1;\n}\n</code></pre> <p>This generates a CLI args class:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport py_gen_ml as pgml\nimport typing\n\nimport pydantic\nimport typer\n\nfrom . import cli_demo_base as base\n\n\nclass CLIDemoArgs(pgml.YamlBaseModel):\n    \"\"\"Global configuration\"\"\"\n\n    path: typing.Annotated[\n        typing.Optional[str],\n        typer.Option(help=\"Path to the dataset. Maps to 'data.dataset.path'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.dataset.path\"),\n    ]\n    \"\"\"Path to the dataset\"\"\"\n\n    num_layers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of layers. Maps to 'model.num_layers'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"model.num_layers\"),\n    ]\n    \"\"\"Number of layers\"\"\"\n\n    num_workers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(\n            help=\"Number of workers for loading the dataset. Maps to 'data.num_workers'\"\n        ),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.num_workers\"),\n    ]\n    \"\"\"Number of workers for loading the dataset\"\"\"\n\n    num_epochs: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of epochs. Maps to 'training.num_epochs'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"training.num_epochs\"),\n    ]\n    \"\"\"Number of epochs\"\"\"\n</code></pre>"},{"location":"guides/cli_argument_parsing/#generated-entrypoint","title":"\ud83d\udeaa Generated Entrypoint","text":"<p>It also generates a skeleton entrypoint:</p> <pre><code>import pgml_out.cli_demo_base as base\nimport pgml_out.cli_demo_sweep as sweep\nimport pgml_out.cli_demo_cli_args as cli_args\nimport typer\nimport py_gen_ml as pgml\nimport optuna\nimport typing\n\napp = typer.Typer(pretty_exceptions_enable=False)\n\ndef run_trial(\n    cli_demo: base.CLIDemo,\n    trial: typing.Optional[optuna.Trial] = None\n) -&gt; typing.Union[float, typing.Sequence[float]]:\n    \"\"\"\n    Run a trial with the given values for cli_demo. The sampled hyperparameters have\n    already been added to the trial.\n    \"\"\"\n    # TODO: Implement this function\n    return 0.0\n\n@pgml.pgml_cmd(app=app)\ndef main(\n    config_paths: typing.List[str] = typer.Option(..., help=\"Paths to config files\"),\n    sweep_paths: typing.List[str] = typer.Option(\n        default_factory=list,\n        help=\"Paths to sweep files\"\n    ),\n    cli_args: cli_args.CLIDemoArgs = typer.Option(...),\n) -&gt; None:\n    cli_demo = base.CLIDemo.from_yaml_files(config_paths)\n    cli_demo = cli_demo.apply_cli_args(cli_args)\n    if len(sweep_paths) == 0:\n        run_trial(cli_demo)\n        return\n    cli_demo_sweep = sweep.CLIDemoSweep.from_yaml_files(sweep_paths)\n\n    def objective(trial: optuna.Trial) -&gt; typing.Union[\n        float,\n        typing.Sequence[float]\n    ]:\n        optuna_sampler = pgml.OptunaSampler(trial)\n        cli_demo_patch = optuna_sampler.sample(cli_demo_sweep)\n        cli_demo_patched = cli_demo.merge(cli_demo_patch)\n        objective_value = run_trial(cli_demo_patched, trial)\n        return objective_value\n\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=100)\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>It is a standard Typer app, so you can run it like a normal Python script:</p> <pre><code>python src/pgml_out/cli_demo_entrypoint.py --help\n</code></pre> <p>Which should show something like:</p> <pre><code> Usage: cli_demo_entrypoint.py [OPTIONS]                                    \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --config-paths              TEXT     Paths to config files            \u2502\n\u2502                                         [default: None]                  \u2502\n\u2502                                         [required]                       \u2502\n\u2502    --sweep-paths               TEXT     Paths to sweep files             \u2502\n\u2502                                         [default: &lt;class 'list'&gt;]        \u2502\n\u2502    --num-epochs                INTEGER  Number of epochs. Maps to        \u2502\n\u2502                                         'num_epochs'                     \u2502\n\u2502                                         [default: None]                  \u2502\n\u2502    --path                      TEXT     Path to the dataset. Maps to     \u2502\n\u2502                                         'path'                           \u2502\n\u2502                                         [default: None]                  \u2502\n\u2502    --num-layers                INTEGER  Number of layers. Maps to        \u2502\n\u2502                                         'num_layers'                     \u2502\n\u2502                                         [default: None]                  \u2502\n\u2502    --num-workers               INTEGER  Number of workers for loading    \u2502\n\u2502                                         the dataset. Maps to             \u2502\n\u2502                                         'num_workers'                    \u2502\n\u2502                                         [default: None]                  \u2502\n\u2502    --install-completion                 Install completion for the       \u2502\n\u2502                                         current shell.                   \u2502\n\u2502    --show-completion                    Show completion for the current  \u2502\n\u2502                                         shell, to copy it or customize   \u2502\n\u2502                                         the installation.                \u2502\n\u2502    --help                               Show this message and exit.      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Notice how the names of the args are just the names of the fields in the innermost message of the nested structure. The names are unique globally, so these short names suffice for finding the intended field within the full structure.</p>"},{"location":"guides/cli_argument_parsing/#workflow","title":"\ud83d\udca1 Workflow","text":"<p>We recommend copying the generated entrypoint and modifying it to fit your needs.</p> <p>For example, you might write a <code>run_trial</code> function that interfaces with your model and training code.</p>"},{"location":"guides/cli_argument_parsing/#shortening-cli-arguments","title":"\u23e9 Shortening CLI arguments","text":"<p>As stated before, CLI argument names are shortened for deeply nested fields in your config when there's exactly one path to a field and the field name is unique. If the field name is not unique, we will prepend accessors to the field name until it is unique. </p> <p>Take for example the following protobuf file:</p> <pre><code>// cli_demo_deep.proto\nsyntax = \"proto3\";\n\npackage cli_demo_deep;\n\nimport \"py_gen_ml/extensions.proto\";\n\n\n// Global configuration\nmessage CliDemoDeep {\n    option (pgml.cli).enable = true;\n    // Dataset configuration\n    Data data = 1;\n    // Model configuration\n    Model model = 2;\n    // Training configuration\n    Training training = 3;\n}\n\n// Dataset configuration\nmessage Dataset {\n    // Path to the dataset\n    string path = 1;\n}\n\n// Data config\nmessage Data {\n    // Path to the dataset\n    Dataset train_dataset = 1;\n    // Path to the dataset\n    Dataset test_dataset = 2;\n    // Number of workers for loading the dataset\n    uint32 num_workers = 3;\n}\n\n// Model configuration\nmessage Model {\n    // Number of layers\n    uint32 num_layers = 1;\n}\n\n// Training configuration\nmessage Training {\n    // Number of epochs\n    uint32 num_epochs = 1;\n}\n</code></pre> <p>This generates the following CLI arguments:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport py_gen_ml as pgml\nimport typing\n\nimport pydantic\nimport typer\n\nfrom . import cli_demo_deep_base as base\n\n\nclass CliDemoDeepArgs(pgml.YamlBaseModel):\n    \"\"\"Global configuration\"\"\"\n\n    train_dataset_path: typing.Annotated[\n        typing.Optional[str],\n        typer.Option(help=\"Path to the dataset. Maps to 'data.train_dataset.path'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.train_dataset.path\"),\n    ]\n    \"\"\"Path to the dataset\"\"\"\n\n    test_dataset_path: typing.Annotated[\n        typing.Optional[str],\n        typer.Option(help=\"Path to the dataset. Maps to 'data.test_dataset.path'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.test_dataset.path\"),\n    ]\n    \"\"\"Path to the dataset\"\"\"\n\n    num_workers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(\n            help=\"Number of workers for loading the dataset. Maps to 'data.num_workers'\"\n        ),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.num_workers\"),\n    ]\n    \"\"\"Number of workers for loading the dataset\"\"\"\n\n    num_epochs: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of epochs. Maps to 'training.num_epochs'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"training.num_epochs\"),\n    ]\n    \"\"\"Number of epochs\"\"\"\n\n    num_layers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of layers. Maps to 'model.num_layers'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"model.num_layers\"),\n    ]\n    \"\"\"Number of layers\"\"\"\n</code></pre> <p>Notice how <code>data.train_dataset.path</code> is shortened to <code>train_dataset_path</code> and <code>data.test_dataset.path</code> is shortened to <code>test_dataset_path</code>.</p>"},{"location":"guides/cli_argument_parsing/#explicit-argument-references","title":"\ud83c\udfaf Explicit Argument References","text":"<p>For more control, use explicit argument references in your protobuf:</p> <pre><code>// cli_extension_demo.proto\nsyntax = \"proto3\";\n\npackage cli_extension_demo;\n\nimport \"py_gen_ml/extensions.proto\";\n\n\n// Global configuration\nmessage CliExtensionDemo {\n    option (pgml.cli) = {\n        enable: true\n        arg: { name: \"train_path\", path: \"data.train_dataset.path\" }\n        arg: { name: \"test_path\", path: \"data.test_dataset.path\" }\n    };\n    // Dataset configuration\n    Data data = 1;\n    // Model configuration\n    Model model = 2;\n    // Training configuration\n    Training training = 3;\n}\n\n// Dataset configuration\nmessage Dataset {\n    // Path to the dataset\n    string path = 1;\n}\n\n// Data config\nmessage Data {\n    // Path to the dataset\n    Dataset train_dataset = 1;\n    // Path to the dataset\n    Dataset test_dataset = 2;\n    // Number of workers for loading the dataset\n    uint32 num_workers = 3;\n}\n\n// Model configuration\nmessage Model {\n    // Number of layers\n    uint32 num_layers = 1;\n}\n\n// Training configuration\nmessage Training {\n    // Number of epochs\n    uint32 num_epochs = 1;\n}\n</code></pre> <p>The explicit argument references will replace the ones we have seen previously:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport py_gen_ml as pgml\nimport typing\n\nimport pydantic\nimport typer\n\nfrom . import cli_extension_demo_base as base\n\n\nclass CliExtensionDemoArgs(pgml.YamlBaseModel):\n    \"\"\"Global configuration\"\"\"\n\n    train_path: typing.Annotated[\n        typing.Optional[str],\n        typer.Option(help=\"Path to the dataset. Maps to 'data.train_dataset.path'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.train_dataset.path\"),\n    ]\n    \"\"\"Path to the dataset\"\"\"\n\n    test_path: typing.Annotated[\n        typing.Optional[str],\n        typer.Option(help=\"Path to the dataset. Maps to 'data.test_dataset.path'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.test_dataset.path\"),\n    ]\n    \"\"\"Path to the dataset\"\"\"\n\n    num_epochs: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of epochs. Maps to 'training.num_epochs'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"training.num_epochs\"),\n    ]\n    \"\"\"Number of epochs\"\"\"\n\n    num_workers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(\n            help=\"Number of workers for loading the dataset. Maps to 'data.num_workers'\"\n        ),\n        pydantic.Field(None),\n        pgml.ArgRef(\"data.num_workers\"),\n    ]\n    \"\"\"Number of workers for loading the dataset\"\"\"\n\n    num_layers: typing.Annotated[\n        typing.Optional[int],\n        typer.Option(help=\"Number of layers. Maps to 'model.num_layers'\"),\n        pydantic.Field(None),\n        pgml.ArgRef(\"model.num_layers\"),\n    ]\n    \"\"\"Number of layers\"\"\"\n</code></pre>"},{"location":"guides/cli_argument_parsing/#summary","title":"\ud83d\udcda Summary","text":"<p>With <code>py-gen-ml</code>, you get powerful, flexible CLI argument parsing that adapts to your needs, whether using implicit shortcuts or explicit references.</p>"},{"location":"guides/defaults/","title":"Default values","text":""},{"location":"guides/defaults/#default-values","title":"\ud83d\udcaf Default Values","text":""},{"location":"guides/defaults/#setting-default-values","title":"\ud83d\udc49 Setting default values","text":"<p>Some configs are unlikely to ever change. In such cases, a default value can be specified.</p> <p>The default needs to be propagated to the generated code. Hence, we'll add the default to the protobuf schema.</p> <pre><code>// default.proto\nsyntax = \"proto3\";\n\npackage defaults;\n\nimport \"py_gen_ml/extensions.proto\";\n\n// Optimizer configuration\nmessage Optimizer {\n    // Optimizer type\n    string type = 1 [(pgml.default).string = \"sgd\"];\n    // Learning rate\n    float learning_rate = 2 [(pgml.default).float = 0.01];\n}\n</code></pre> <p>The default value will be added to the generated code.</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport py_gen_ml as pgml\n\n\nclass Optimizer(pgml.YamlBaseModel):\n    \"\"\"Optimizer configuration\"\"\"\n\n    type: str = \"sgd\"\n    \"\"\"Optimizer type\"\"\"\n\n    learning_rate: float = 0.01\n    \"\"\"Learning rate\"\"\"\n</code></pre> <p>In this case, all values have a default, so it is possible to instantiate the class without specifying any values.</p> <pre><code>from pgml_out.default_base import Optimizer\n\noptimizer = Optimizer()\n</code></pre>"},{"location":"guides/defaults/#enums","title":"\ud83d\udd20 Enums","text":"<p>Enum values can be specified using the name of the enum value.</p> <pre><code>// default_enum.proto\nsyntax = \"proto3\";\n\npackage defaults_enum;\n\nimport \"py_gen_ml/extensions.proto\";\n\n// Activation function\nenum Activation {\n    // Rectified Linear Unit\n    RELU = 0;\n    // Gaussian Error Linear Unit\n    GELU = 1;\n}\n\n// Linear layer\nmessage Linear {\n    // Number of input features\n    int32 in_features = 1;\n    // Number of output features\n    int32 out_features = 2;\n    // Activation function\n    Activation activation = 3 [(pgml.default).enum = \"GELU\"];\n}\n</code></pre> <pre><code># Autogenerated code. DO NOT EDIT.\nimport enum\n\nimport py_gen_ml as pgml\n\n\nclass Activation(str, enum.Enum):\n    \"\"\"Activation function\"\"\"\n\n    RELU = \"RELU\"\n    \"\"\"Rectified Linear Unit\"\"\"\n\n    GELU = \"GELU\"\n    \"\"\"Gaussian Error Linear Unit\"\"\"\n\n\nclass Linear(pgml.YamlBaseModel):\n    \"\"\"Linear layer\"\"\"\n\n    in_features: int\n    \"\"\"Number of input features\"\"\"\n\n    out_features: int\n    \"\"\"Number of output features\"\"\"\n\n    activation: Activation = Activation.GELU\n    \"\"\"Activation function\"\"\"\n</code></pre>"},{"location":"guides/defaults/#limitations","title":"\ud83d\udea7 Limitations","text":"<p>It is currently only possible to specify defaults for built-ins such as <code>string</code>, <code>float</code>, <code>int</code>, etc. For message fields, you cannot specify a default value. We leave this feature for future work.</p>"},{"location":"guides/defining_yaml_files/","title":"YAML configuration","text":""},{"location":"guides/defining_yaml_files/#defining-yaml-files","title":"\ud83d\udcdd Defining YAML Files","text":"<p>YAML files are the backbone of your project's configuration in <code>py-gen-ml</code>. To make working with these files a breeze, <code>py-gen-ml</code> automatically generates JSON schemas for each protobuf model. These schemas are your secret weapon for validating YAML files with ease!</p>"},{"location":"guides/defining_yaml_files/#default-project-structure","title":"\ud83c\udfd7\ufe0f Default Project Structure","text":"<p>When you use <code>py-gen-ml</code>, it sets up a neat and organized structure for your schemas:</p> <pre><code>&lt;project_root&gt;/\n    configs/\n        base/\n            schemas/\n                &lt;message_name_a&gt;.json\n                &lt;message_name_b&gt;.json\n                ...\n        patch/\n            schemas/\n                &lt;message_name_a&gt;.json\n                &lt;message_name_b&gt;.json\n                ...\n        sweep/\n            schemas/\n                &lt;message_name_a&gt;.json\n                &lt;message_name_b&gt;.json\n                ...\n</code></pre>"},{"location":"guides/defining_yaml_files/#putting-schemas-to-work","title":"\ud83d\udee0\ufe0f Putting Schemas to Work","text":"<p>Want to leverage these schemas in Visual Studio Code? It's simple! Just install the YAML plugin and add this line to the top of your YAML file:</p> <pre><code># yaml-language-server: $schema=schemas/&lt;message_name&gt;.json\n</code></pre> <p>(We're assuming your file is located under <code>&lt;project_root&gt;/configs/base/</code>.)</p> <p>Let's take the following proto as an example:</p> <pre><code>syntax = \"proto3\";\n\npackage mlp;\n\nimport \"py_gen_ml/extensions.proto\";\n\n// Activation is an enum of activation functions.\nenum Activation {\n    // ReLU is the Rectified Linear Unit activation function.\n    RELU = 0;\n    // TANH is the hyperbolic tangent activation function.\n    TANH = 1;\n    // SIGMOID is the sigmoid activation function.\n    SIGMOID = 2;\n}\n\n// MLP is a simple multi-layer perceptron.\nmessage MLPParsingDemo {\n    // Number of layers in the MLP.\n    uint32 num_layers = 1 [(pgml.default).uint32 = 2];\n    // Number of units in each layer.\n    uint32 num_units = 2;\n    // Activation function to use.\n    Activation activation = 3;\n}\n</code></pre> <p>Here's a quick example of what your YAML file might look like:</p> <pre><code># yaml-language-server: $schema=schemas/mlp_parsing_demo.json\nactivation: tanh\nnum_layers: 2\nnum_units: 32\n</code></pre> <p>Now, if you accidentally misconfigure your YAML file, Visual Studio Code will give you a friendly heads-up with a validation error.</p> <p>The video below shows how the editor leverages the schema to know exactly which fields can be added and when the field is invalid:</p> <p></p> <p>You can see that:</p> <ol> <li>The leading comment we have added to the message in the proto shows at the top of the file.</li> <li>By pressing Cmd+Space (or Ctrl+Space on Linux) with an empty file, we see the list of possible fields.</li> <li>By pressing Cmd+Space (or Ctrl+Space on Linux) after typing <code>activation:</code>, we get a list of possible values for the field.</li> <li>By entering an invalid value for <code>num_units</code>, we get a validation error.</li> </ol>"},{"location":"guides/defining_yaml_files/#handling-nested-messages","title":"\ud83e\udde9 Handling Nested Messages","text":"<p>Let's kick things up a notch with a more complex protobuf that includes some nesting:</p> <pre><code>// advanced.proto\nsyntax = \"proto3\";\n\npackage advanced;\n\nimport \"py_gen_ml/extensions.proto\";\n\n// Linear block configuration\nmessage LinearBlock {\n    // Number of units\n    int32 num_units = 1;\n    // Activation function\n    string activation = 2;\n}\n\n// Multi-layer perceptron configuration\nmessage MLP {\n    // List of linear blocks\n    repeated LinearBlock layers = 1;\n}\n\n// Optimizer configuration\nmessage Optimizer {\n    // Type of optimizer\n    string type = 1;\n    // Learning rate\n    float learning_rate = 2;\n}\n\n// Training configuration\nmessage Training {\n    option (pgml.cli).enable = true;\n    // Multi-layer perceptron configuration\n    MLP mlp = 1;\n    // Optimizer configuration\n    Optimizer optimizer = 2;\n}\n</code></pre> <p>You can define a YAML file for this structure like so:</p> <pre><code># configs/base/default.yaml\n# yaml-language-server: $schema=schemas/training.json\nmlp:\n  layers:\n    - num_units: 100\n      activation: relu\n    - num_units: 200\n      activation: relu\n    - num_units: 100\n      activation: relu\noptimizer:\n  type: sgd\n  learning_rate: 0.01\n</code></pre> <p>As you can see, the nesting in the YAML file mirrors the nesting in the protobuf.</p> <p>Now, let's put this config to work by creating a model and an optimizer:</p> <pre><code>from pgml_out.advanced_base import Training\n\ndef create_model(config: Training) -&gt; torch.nn.Module:\n    layers = []\n    for layer in config.mlp.layers:\n        layers.append(torch.nn.Linear(layer.num_units, layer.num_units))\n        layers.append(torch.nn.ReLU() if layer.activation == \"relu\" else torch.nn.Tanh())\n    return torch.nn.Sequential(*layers)\n\ndef create_optimizer(model: torch.nn.Module, config: Training) -&gt; torch.optim.Optimizer:\n    return torch.optim.SGD(model.parameters(), lr=config.optimizer.learning_rate)\n\nif __name__ == \"__main__\":\n    config = Training.from_yaml_file(\"configs/base/default.yaml\")\n    model = create_model(config)\n    optimizer = create_optimizer(model, config)\n</code></pre>"},{"location":"guides/defining_yaml_files/#internal-references-with","title":"\ud83d\udd17 Internal References with <code>#</code>","text":"<p>Want to reuse values in your YAML file? <code>py-gen-ml</code> has got you covered! You can replace a value with a reference to another value using the <code>#&lt;path_to_value&gt;</code> syntax. Here's how it works:</p> <pre><code># configs/base/default.yaml\n# yaml-language-server: $schema=schemas/training.json\nmlp:\n  layers:\n    - num_units: 100\n      activation: relu\n    - num_units: \"#/mlp/layers[0]/num_units\"\n      activation: \"#/mlp/layers[0]/activation\"\n    - num_units: \"#/mlp/layers[0]/num_units\"\n      activation: \"#/mlp/layers[0]/activation\"\noptimizer:\n  type: sgd\n  learning_rate: 0.01\n</code></pre> <p>In this example, the second and third layers will mirror the number of units and activation function of the first layer. </p>"},{"location":"guides/defining_yaml_files/#using-the-_defs_-field","title":"\ud83c\udfaf Using the <code>_defs_</code> Field","text":"<p>For even more flexibility, you can use the <code>_defs_</code> field. It's perfect for reusing values with shorter paths and a more centralized definition:</p> <pre><code># configs/base/default.yaml\n# yaml-language-server: $schema=schemas/training.json\nmlp:\n  layers:\n    - '#/_defs_/layer'\n    - '#/_defs_/layer'\n    - '#/_defs_/layer'\noptimizer:\n  type: sgd\n  learning_rate: 0.01\n_defs_:\n  layer:\n    num_units: 100\n    activation: relu\n</code></pre>"},{"location":"guides/defining_yaml_files/#using-indices-in-lists","title":"\ud83d\udcca Using Indices in Lists","text":"<p>Need to reference specific elements in a list? No problem! You can use indices like this:</p> <pre><code># configs/base/default.yaml\n# yaml-language-server: $schema=schemas/training.json\nmlp:\n  layers:\n    - num_units: 100\n      activation: relu\n    - '#/mlp/layers[2]'\n    - num_units: 200\n      activation: relu\noptimizer:\n  type: sgd\n  learning_rate: 0.01\n</code></pre>"},{"location":"guides/defining_yaml_files/#relative-internal-references","title":"\ud83d\udc6a Relative Internal References","text":"<p>You can also use relative internal references. This is useful if you want to reuse values in a nested structure and the reference is close to the reused value.</p> <pre><code># configs/base/default.yaml\n# yaml-language-server: $schema=schemas/training.json\nfoo:\n  bar:\n    data:\n      train_dataset:\n        path: train.csv\n        batch_size: 32\n      test_dataset:\n        path: test.csv\n        batch_size: '#../train_dataset/batch_size'\noptimizer:\n  type: sgd\n  learning_rate: 0.01\n</code></pre> <p>This allows you to skip the <code>/foo/bar</code> prefix to get to the <code>data</code> field. It also makes this part of the YAML file more self-contained: you can safely copy this part to a different YAML that follows a different schema yet the same relative structure for the <code>data</code> field.</p>"},{"location":"guides/defining_yaml_files/#external-references-with","title":"\ud83c\udf10 External References with <code>!</code>","text":"<p>Want to reuse values across multiple YAML files? External references by prefixing the path with <code>!</code> is the way to go:</p> <pre><code># configs/base/default.yaml\n# yaml-language-server: $schema=schemas/training.json\nmlp:\n  layers:\n    - '!layer.yaml'\n    - '!layer.yaml'\n    - '!layer.yaml'\noptimizer: '!optimizer.yaml'\n</code></pre> <p>The referenced files might look like this:</p> <pre><code># configs/base/layer.yaml\nnum_units: 100\nactivation: relu\n</code></pre> <pre><code># configs/base/optimizer.yaml\ntype: sgd\nlearning_rate: 0.01\n</code></pre>"},{"location":"guides/defining_yaml_files/#combining-external-and-internal-references","title":"\ud83d\udd00 Combining External and Internal References","text":"<p>For the ultimate flexibility, you can mix and match external and internal references:</p> <pre><code># configs/base/default.yaml\n# yaml-language-server: $schema=schemas/training.json\nmlp:\n  layers:\n    - '!layer.yaml#/layer0'\n    - '!layer.yaml#/layer1'\n    - '!layer.yaml#/layer2'\noptimizer:\n  type: sgd\n  learning_rate: 0.01\n</code></pre> <p>With the corresponding <code>layer.yaml</code>:</p> <pre><code># configs/base/layer.yaml\nlayer0:\n    num_units: 100\n    activation: relu\nlayer1:\n    num_units: 200\n    activation: relu\nlayer2:\n    num_units: 100\n    activation: relu\n</code></pre> <p>And there you have it! With these powerful YAML configuration techniques at your fingertips, you're all set to create flexible and maintainable machine learning projects using <code>py-gen-ml</code>. Happy coding! \ud83d\ude80</p>"},{"location":"guides/enums/","title":"Enums","text":""},{"location":"guides/enums/#enums","title":"\ud83d\udd20 Enums","text":"<p>Enums can be used to represent a set of named values that can be assigned to a field.</p> <p>Protobuf provides a dedicated syntax for defining enums. </p><pre><code>// enum_demo.proto\nsyntax = \"proto3\";\n\npackage enum_demo;\n\n// Activation function\nenum Activation {\n    // ReLU activation\n    RELU = 0;\n    // Gelu activation\n    GELU = 1;\n}\n\n// MLP configuration\nmessage MLP {\n    // Activation function\n    Activation activation = 1;\n    // Number of layers\n    uint32 num_layers = 2;\n}\n</code></pre> <p>The generated code will look like this:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport enum\n\nimport py_gen_ml as pgml\n\n\nclass Activation(str, enum.Enum):\n    \"\"\"Activation function\"\"\"\n\n    RELU = \"RELU\"\n    \"\"\"ReLU activation\"\"\"\n\n    GELU = \"GELU\"\n    \"\"\"Gelu activation\"\"\"\n\n\nclass MLP(pgml.YamlBaseModel):\n    \"\"\"MLP configuration\"\"\"\n\n    activation: Activation\n    \"\"\"Activation function\"\"\"\n\n    num_layers: int\n    \"\"\"Number of layers\"\"\"\n</code></pre>"},{"location":"guides/example-cli/","title":"example","text":""},{"location":"guides/example-cli/#example","title":"<code>example</code>","text":"<p>Usage:</p> <pre><code>$ example [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--other-arg TEXT</code>: [default: other_arg]</li> <li><code>--config-path TEXT</code>: [default: example.yaml]</li> <li><code>--num-layers INTEGER</code>: Number of layers. Maps to 'num_layers'</li> <li><code>--num-units INTEGER</code>: Number of units. Maps to 'num_units'</li> <li><code>--activation TEXT</code>: Activation function. Maps to 'activation'</li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"guides/oneofs/","title":"Unions","text":""},{"location":"guides/oneofs/#unions","title":"\ud83e\uddc5 Unions","text":"<p>To allow for a union of types, you can use the protobuf <code>oneof</code> keyword.</p> <pre><code>// oneof_demo.proto\nsyntax = \"proto3\";\n\npackage oneof_demo;\n\n// Transformer configuration\nmessage Transformer {\n    // Number of layers\n    uint32 num_layers = 1;\n    // Number of heads\n    uint32 num_heads = 2;\n    // Activation function\n    string activation = 3;\n}\n\n// Conv block\nmessage ConvBlock {\n    // Number of output channels\n    uint32 out_channels = 1;\n    // Kernel size\n    uint32 kernel_size = 2;\n    // Activation function\n    string activation = 3;\n}\n\n// Convolutional neural network configuration\nmessage ConvNet {\n    // Conv layer configuration\n    repeated ConvBlock layers = 1;\n}\n\n// Model configuration\nmessage Model {\n    oneof backbone {\n        Transformer transformer = 1;\n        ConvNet conv_net = 2;\n    }\n}\n</code></pre> <p>The generated code will look like this:</p> <pre><code># Autogenerated code. DO NOT EDIT.\nimport typing\nimport py_gen_ml as pgml\n\n\nclass Transformer(pgml.YamlBaseModel):\n    \"\"\"Transformer configuration\"\"\"\n\n    num_layers: int\n    \"\"\"Number of layers\"\"\"\n\n    num_heads: int\n    \"\"\"Number of heads\"\"\"\n\n    activation: str\n    \"\"\"Activation function\"\"\"\n\n\nclass ConvBlock(pgml.YamlBaseModel):\n    \"\"\"Conv block\"\"\"\n\n    out_channels: int\n    \"\"\"Number of output channels\"\"\"\n\n    kernel_size: int\n    \"\"\"Kernel size\"\"\"\n\n    activation: str\n    \"\"\"Activation function\"\"\"\n\n\nclass ConvNet(pgml.YamlBaseModel):\n    \"\"\"Convolutional neural network configuration\"\"\"\n\n    layers: typing.List[ConvBlock]\n    \"\"\"Conv layer configuration\"\"\"\n\n\nclass Model(pgml.YamlBaseModel):\n    \"\"\"Model configuration\"\"\"\n\n    backbone: typing.Union[Transformer, ConvNet]\n</code></pre>"},{"location":"guides/patching/","title":"Patching","text":""},{"location":"guides/patching/#patching","title":"\ud83e\udde9 Patching","text":"<p>YAML files can be patched together to manage smaller changes without having to copy or re-write the entire configuration. This approach is particularly useful for temporary changes targeted at a single run or a small set of runs. Patches allow you to conceptualize experiments in terms of changes relative to a baseline configuration, rather than considering the full configuration each time.</p>"},{"location":"guides/patching/#json-schemas","title":"\ud83d\udcca JSON Schemas","text":"<p>When you set up a protobuf schema and run <code>py-gen-ml</code>, it generates three types of JSON schemas:</p> <pre><code>&lt;project_root&gt;/\n    configs/\n        base/\n            schemas/\n                &lt;message_name_a&gt;.json\n                &lt;message_name_b&gt;.json\n                ...\n        patch/\n            schemas/\n                &lt;message_name_a&gt;.json\n                &lt;message_name_b&gt;.json\n                ...\n        sweep/\n            schemas/\n                &lt;message_name_a&gt;.json\n                &lt;message_name_b&gt;.json\n                ...\n</code></pre> <p>In the previous guide, we explored how to leverage the base schemas for validation in your IDE.</p>"},{"location":"guides/patching/#pydantic-models","title":"\ud83c\udfd7\ufe0f Pydantic Models","text":"<p>The patch schemas are generated from the patch Pydantic models, which contain all fields of the base model but modified to be optional. This allows you to specify only the fields you want to change in your patch.</p> <p>Let's examine an example.</p> <ol> <li> <p>The proto </p><pre><code>// quickstart_a.proto\nsyntax = \"proto3\";\n\npackage example;\n\n// Multi-layer perceptron configuration\nmessage MLPQuickstart {\n    // Number of layers\n    int64 num_layers = 1;\n    // Number of units\n    int64 num_units = 2;\n    // Activation function\n    string activation = 3;\n}\n</code></pre> </li> <li> <p>The generated base model </p><pre><code># Autogenerated code. DO NOT EDIT.\nimport py_gen_ml as pgml\n\n\nclass MLPQuickstart(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: int\n    \"\"\"Number of layers\"\"\"\n\n    num_units: int\n    \"\"\"Number of units\"\"\"\n\n    activation: str\n    \"\"\"Activation function\"\"\"\n</code></pre> </li> <li> <p>The generated patch model </p><pre><code># Autogenerated code. DO NOT EDIT.\nimport typing\nimport py_gen_ml as pgml\n\n\nclass MLPQuickstartPatch(pgml.YamlBaseModel):\n    \"\"\"Multi-layer perceptron configuration\"\"\"\n\n    num_layers: typing.Optional[int] = None\n    \"\"\"Number of layers\"\"\"\n\n    num_units: typing.Optional[int] = None\n    \"\"\"Number of units\"\"\"\n\n    activation: typing.Optional[str] = None\n    \"\"\"Activation function\"\"\"\n</code></pre> </li> </ol> <p>As you can observe in (3), we can choose to omit any fields that we don't want to change, as their defaults are set to <code>None</code>.</p>"},{"location":"guides/patching/#defining-a-patch","title":"\ud83c\udfa8 Defining a Patch","text":"<p>Consider this baseline config:</p> <pre><code># configs/base/mlp.yaml\n# yaml-language-server: $schema=schemas/mlp.json\nnum_layers: 3\nnum_units: 100\nactivation: relu\n</code></pre> <p>If you want to run an experiment where you change the number of layers to 4, you can create the following patch:</p> <pre><code># configs/patch/mlp_num_layers.yaml\n# yaml-language-server: $schema=schemas/mlp_patch.json\nnum_layers: 4\n</code></pre>"},{"location":"guides/patching/#loading-a-patch","title":"\ud83d\udd27 Loading a Patch","text":"<p>In your script, you can load the base config and the patch config and merge them together:</p> <pre><code>from pgml_out.proto_intro_base import MLP\n\nconfig = MLP.from_yaml_files([\n    \"configs/base/mlp.yaml\",\n    \"configs/patch/mlp_num_layers.yaml\",\n])\n</code></pre> <p>The <code>from_yaml_files</code> function will merge the patch into the base config using <code>jsonmerge</code>.</p>"},{"location":"guides/patching/#loading-a-patch-separately","title":"\ud83d\udd0d Loading a Patch Separately","text":"<p>If you need to load the patch separately from the base config, you can use the <code>from_yaml_file</code> method with the patch Pydantic model:</p> <pre><code>patch = MLPPatch.from_yaml_file(\"configs/patch/mlp_num_layers.yaml\")\n</code></pre> <p>You can then merge the patch with the base config in your script:</p> <pre><code>config = MLP.from_yaml_file(\"configs/base/mlp.yaml\")\npatched_config = config.merge(patch)\n</code></pre> <p>The <code>merge</code> method also patches the config using <code>jsonmerge</code>.</p> <p>By utilizing these patching techniques, you can efficiently manage and experiment with your configurations.</p>"},{"location":"guides/protobuf/","title":"Protobuf crash course","text":""},{"location":"guides/protobuf/#understanding-protobuf-in-py-gen-ml","title":"\ud83e\udde0 Understanding Protobuf in py-gen-ml","text":""},{"location":"guides/protobuf/#introduction","title":"\ud83c\udf1f Introduction","text":"<p>In this guide, we'll explore the basics of protobuf and how it integrates with the <code>py-gen-ml</code> library.</p>"},{"location":"guides/protobuf/#what-is-protobuf","title":"\ud83d\udd0d What is Protobuf?","text":"<p>Protobuf (Protocol Buffers) is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. It's a powerful tool for defining data structures and for serializing and deserializing data across different programming languages.</p> <p>When you install <code>py-gen-ml</code> via <code>pip</code>, you get a protobuf compiler plugin called <code>protoc-gen-py-ml</code>. This plugin works behind the scenes when you run <code>py-gen-ml</code> to generate code. You do not have to use the <code>protoc-gen-py-ml</code> plugin directly.</p>"},{"location":"guides/protobuf/#what-does-py-gen-ml-generate","title":"\ud83d\udee0\ufe0f What does py-gen-ml generate?","text":"<p><code>py-gen-ml</code> creates several Pydantic models based on your schema:</p> <ol> <li>A 'base' model that closely follows the protobuf schema</li> <li>A 'patch' model for overlaying a base model with modifications</li> <li>A 'sweep' model for defining parameter sweeps over the base model</li> <li>A CLI model that enables automatic argument parsing with nested field support</li> </ol>"},{"location":"guides/protobuf/#why-use-protobuf-instead-of-pydantic-directly","title":"\ud83e\udd14 Why use Protobuf instead of Pydantic directly?","text":"<p>You might wonder why we chose protobuf over direct Pydantic models to act as the source of truth for data structures. Here's why:</p> <ul> <li>\ud83e\udde9 Separation of Concerns: Protobuf separates data structure definition from logic implementation.</li> <li>\ud83d\udd12 Atomic Code Changes: Generating from a schema instead of code ensures 100% of the code is generated, reducing the impact of divergence between the source of truth and the generated code.</li> <li>\ud83c\udf10 Rich Ecosystem: Protobuf's extensive toolset opens up possibilities for future enhancements.</li> </ul>"},{"location":"guides/protobuf/#key-concepts","title":"\ud83e\uddf1 Key Concepts","text":"<p>Let's break down the main components of Protobuf that you'll need to know:</p>"},{"location":"guides/protobuf/#message","title":"\ud83d\udce6 Message","text":"<p>A message is a collection of fields, similar to a <code>dataclass</code> or a Pydantic <code>BaseModel</code>. Here's the basic syntax:</p> <pre><code>message MessageName {\n    FieldType FieldName = FieldNumber;\n}\n</code></pre> <p>For example:</p> <pre><code>message Dog {\n    string name = 1;\n    uint32 age = 2;\n    string breed = 3;\n}\n</code></pre> <p>Info</p> <p>The term 'message' comes from protobuf's origin in data transfer. The message is serialized before being sent and deserialized after being received. A protobuf compiler generates the code to serialize and deserialize the message for a wide variety of languages.</p>"},{"location":"guides/protobuf/#field","title":"\ud83c\udff7\ufe0f Field","text":"<p>A field consists of a type, a name, and a number. The field number is a unique identifier within the message.</p> <p>Info</p> <p>The field numbers must be unique. They are used to make the serialized representation agnostic to field names. This allows a sender and receiver to change field names independently without breaking the serialized format. If using the protobufs purely for use cases that <code>py-gen-ml</code> supports, you can ignore this detail. The main take away is that field numbers are required to be unique within the message.</p>"},{"location":"guides/protobuf/#built-in-types","title":"\ud83d\udcca Built-in Types","text":"<p>Protobuf offers various built-in types:</p> Type Description <code>double</code> 64-bit float <code>float</code> 32-bit float <code>int32</code> 32-bit signed integer <code>int64</code> 64-bit signed integer <code>uint32</code> Unsigned 32-bit integer <code>uint64</code> Unsigned 64-bit integer <code>bool</code> Boolean value <code>string</code> String of characters <code>bytes</code> Sequence of bytes <p>This list is not exhaustive, but should be enough to use <code>py-gen-ml</code> effectively. For more types see the protobuf docs.</p>"},{"location":"guides/protobuf/#nesting","title":"\ud83e\ude86 Nesting","text":"<p>Messages can be nested within other messages:</p> <pre><code>message Address {\n    string street = 1;\n    string city = 2;\n    string state = 3;\n    string zip = 4;\n}\n\nmessage Person {\n    string name = 1;\n    uint32 age = 2;\n    Address address = 3;\n}\n</code></pre>"},{"location":"guides/protobuf/#oneof","title":"\ud83d\udd00 Oneof","text":"<p>A oneof is a set of mutually exclusive fields:</p> <pre><code>message Owner {\n    string name = 1;\n    oneof pet {\n        Dog dog = 2;\n        Cat cat = 3;\n    }\n}\n</code></pre>"},{"location":"guides/protobuf/#repeated","title":"\ud83d\udd01 Repeated","text":"<p>A repeated field contains a list of values:</p> <pre><code>message Owner {\n    repeated Pet pets = 1;\n}\n</code></pre>"},{"location":"guides/protobuf/#optional","title":"\u2753 Optional","text":"<p>An optional field may or may not be present:</p> <p></p><pre><code>message Pet {\n    string name = 1;\n    optional string owner_name = 2;\n}\n</code></pre> If a field is optional, it will be translated to a <code>typing.Optional</code> type in Pydantic with the default value set to <code>None</code>."},{"location":"guides/protobuf/#enum","title":"\ud83c\udfa8 Enum","text":"<p>An enum is a type with a predefined set of values:</p> <pre><code>enum Color {\n    RED = 0;\n    GREEN = 1;\n    BLUE = 2;\n}\n\nmessage Car {\n    Color color = 1;\n}\n</code></pre>"},{"location":"guides/protobuf/#adding-commentsbuf","title":"\ud83d\udcac Adding Commentsbuf","text":"<p>Use <code>//</code> for comments in your proto files:</p> <pre><code>// A car has a color\nmessage Car {\n    // The color of the car\n    Color color = 1;\n}\n</code></pre> <p>For <code>py-gen-ml</code>, leading comments are preserved in the generated code, while trailing comments are not.</p>"},{"location":"guides/protobuf/#wrapping-up","title":"\ud83d\udcda Wrapping up","text":"<p>Now you're equipped with the basics of Protobuf in <code>py-gen-ml</code>! Happy coding! \ud83d\ude80</p> <p>Note</p> <p>To learn more about the internals of protobuf, here are some optional references to dive into:</p> <ul> <li>Protobuf in Python</li> <li>Protobuf Documentation</li> <li>Protobuf Python API Reference</li> </ul>"},{"location":"guides/sweep/","title":"Sweeps","text":""},{"location":"guides/sweep/#parameter-sweeping","title":"\ud83d\udd0d Parameter Sweeping","text":"<p>For parameter sweeps, <code>py-gen-ml</code> generates a Pydantic base model that replaces the types in the original config with structures that allow for defining the sampling space for each parameter.</p> <p>The sweep config is then passed to a <code>py_gen_ml.OptunaSampler</code> which will sample the parameter space and return a patch that can be applied to a base config.</p> <p>Your training code shouldn't have to be changed for a parameter sweep. It will receive the modified config as input and can remain oblivious to the fact that it has been sampled from a larger space.</p>"},{"location":"guides/sweep/#defining-a-parameter-sweep","title":"\ud83e\uddea Defining a parameter sweep","text":"<p>Let's do a benchmark on how to iterate throug a <code>torch.utils.data.DataLoader</code> as fast as possible.</p>"},{"location":"guides/sweep/#the-schema","title":"\ud83d\udd27 The schema","text":"<p>We will define a simple schema with some parameter that influence the dataloader.</p> <pre><code>// dataloader.proto\nsyntax = \"proto3\";\n\npackage dataloader;\n\nimport \"py_gen_ml/extensions.proto\";\n\n// DataLoader configuration\nmessage DataLoaderConfig {\n    option (pgml.cli).enable = true;\n    // Batch size\n    int32 batch_size = 1;\n    // Number of workers\n    int32 num_workers = 2;\n    // Pin memory\n    bool pin_memory = 3;\n    // Persistent workers\n    bool persistent_workers = 4;\n    // Prefetch factor\n    int32 prefetch_factor = 5;\n}\n</code></pre> <p>When we run <code>py-gen-ml</code> it will generate a Pydantic model for parameter sweeps for us.</p> <pre><code>import typing\n\nimport py_gen_ml as pgml\n\nfrom . import dataloader_patch as patch\nfrom . import dataloader_base as base\n\n\nclass DataLoaderConfigSweep(pgml.Sweeper[patch.DataLoaderConfigPatch]):\n    \"\"\"DataLoader configuration\"\"\"\n\n    batch_size: typing.Optional[pgml.IntSweep] = None\n    \"\"\"Batch size\"\"\"\n\n    num_workers: typing.Optional[pgml.IntSweep] = None\n    \"\"\"Number of workers\"\"\"\n\n    pin_memory: typing.Optional[pgml.BoolSweep] = None\n    \"\"\"Pin memory\"\"\"\n\n    persistent_workers: typing.Optional[pgml.BoolSweep] = None\n    \"\"\"Persistent workers\"\"\"\n\n    prefetch_factor: typing.Optional[pgml.IntSweep] = None\n    \"\"\"Prefetch factor\"\"\"\n\n\n\nDataLoaderConfigSweepField = typing.Union[\n    DataLoaderConfigSweep,\n    pgml.NestedChoice[DataLoaderConfigSweep, patch.DataLoaderConfigPatch],  # type: ignore\n]\n</code></pre> <p>You can see that it replaced the types in the original config with structures that allow for defining the sampling space for each parameter. The <code>pgml.IntSweep</code> type allows for several sampling strategies:</p> <ol> <li>Uniform sampling: sample uniformly from a range by specifying <code>low</code>, <code>high</code> and optionally <code>step</code>.</li> <li>Discrete sampling: sample from a list of discrete values by specifying <code>options</code>.</li> </ol> <p>The <code>pgml.BoolSweep</code> type allows for sampling from a boolean space.</p>"},{"location":"guides/sweep/#the-base-config","title":"\u26be The base config","text":"<p>To run a benchmark we need a base config. Any sweeps will be applied to the base config by overlaying the sampled parameters.</p> <p>The default YAML config is given below: </p><pre><code>batch_size: 32\nprefetch_factor: 2\npin_memory: true\npersistent_workers: true\nnum_workers: 2\n</code></pre>"},{"location":"guides/sweep/#the-script","title":"\u270d\ufe0f The script","text":"<p>We will load this config in the following script:</p> <pre><code>@app.command()\ndef run(\n    config_paths: List[str] = typer.Option(..., help='Paths to config files'),\n    sweep_paths: List[str] = typer.Option(default_factory=list, help='Paths to sweep files'),\n    num_trials: int = typer.Option(50, help='Number of trials to run')\n) -&gt; None:\n    dataloader_config = DataLoaderConfig.from_yaml_files(config_paths)\n\n    if len(sweep_paths) == 0:\n        dataloader_speed_benchmark(dataloader_config, num_iters=10)\n        return\n\n    sweep_config = DataLoaderConfigSweep.from_yaml_files(sweep_paths)\n\n    def objective(trial: optuna.Trial) -&gt; float:\n        sampler = pgml.OptunaSampler(trial=trial)\n        patch = sampler.sample(sweep_config)\n        return dataloader_speed_benchmark(dataloader_config.merge(patch), num_iters=10)\n\n    study = optuna.create_study(storage='sqlite:///sweep_dataloader.db', direction='minimize', pruner=RepeatPruner())\n    study.optimize(objective, n_trials=num_trials)\n    print(f'Best value: {study.best_value} (params: {study.best_params})')\n</code></pre> <ul> <li>Line 3: the path to the config can be passed as a CLI option</li> <li>Line 7: we parse the config file</li> <li>line 9, 10: if there is no sweep file given, we run a benchmark on the base config</li> </ul>"},{"location":"guides/sweep/#the-sweep-config","title":"\ud83d\udee0\ufe0f The sweep config","text":"<p>Next, we'll define a minimalistic sweep config to sweep over the batch size.</p> <pre><code># yaml-language-server: $schema=schemas/data_loader_config.json\nbatch_size:\n  options:\n  - 32\n  - 64\n</code></pre> <p>In the <code>run</code> function we load this sweep config and set a few things related to Optuna.</p> <pre><code>@app.command()\ndef run(\n    config_paths: List[str] = typer.Option(..., help='Paths to config files'),\n    sweep_paths: List[str] = typer.Option(default_factory=list, help='Paths to sweep files'),\n    num_trials: int = typer.Option(50, help='Number of trials to run')\n) -&gt; None:\n    dataloader_config = DataLoaderConfig.from_yaml_files(config_paths)\n\n    if len(sweep_paths) == 0:\n        dataloader_speed_benchmark(dataloader_config, num_iters=10)\n        return\n\n    sweep_config = DataLoaderConfigSweep.from_yaml_files(sweep_paths)\n\n    def objective(trial: optuna.Trial) -&gt; float:\n        sampler = pgml.OptunaSampler(trial=trial)\n        patch = sampler.sample(sweep_config)\n        return dataloader_speed_benchmark(dataloader_config.merge(patch), num_iters=10)\n\n    study = optuna.create_study(storage='sqlite:///sweep_dataloader.db', direction='minimize', pruner=RepeatPruner())\n    study.optimize(objective, n_trials=num_trials)\n    print(f'Best value: {study.best_value} (params: {study.best_params})')\n</code></pre> <ul> <li>Line 4: add a CLI option for the sweep config</li> <li>Line 13: load the sweep config</li> <li>Line 15-18: define the objective function. This is the function that will be optimized. It takes a <code>trial</code> object, samples the parameters for the dataloader and returns the result of the benchmark.</li> <li>Line 20: create a new study</li> <li>Line 21: run the study for a given amount of trials</li> </ul> <p>We can now run the sweep with the following command:</p> <pre><code>python sweep_dataloader.py \\\n  --config_paths \\\n  configs/base/default.yaml \\\n  --sweep_paths \\\n  configs/sweep/batch_size.yaml \\\n  --num_trials 2\n</code></pre> <p>You will see something like the following:</p> <pre><code>[I 2024-10-07 11:10:54,448] A new study created in RDB with name: no-name-724460b6-177e-4750-b046-15627aad8711\nFiles already downloaded and verified\nTime taken: 1.173576545715332\n[I 2024-10-07 11:11:07,083] Trial 0 finished with value: 1.173576545715332 and parameters: {'batch_size': 64}. Best is trial 0 with value: 1.173576545715332.\nFiles already downloaded and verified\nTime taken: 1.3076978921890259\n[I 2024-10-07 11:11:21,024] Trial 1 finished with value: 1.3076978921890259 and parameters: {'batch_size': 32}. Best is trial 0 with value: 1.173576545715332.\nBest value: 1.173576545715332 (params: {'batch_size': 64})\n</code></pre>"},{"location":"guides/sweep/#benchmark","title":"\ud83d\udcbb Benchmark","text":"<p>The code that actually runs the benchmark is the following:</p> <pre><code>def dataloader_speed_benchmark(config: DataLoaderConfig, num_iters: int = 100) -&gt; float:\n    torch.cuda.empty_cache()\n    dataloader = torch.utils.data.DataLoader(\n        get_dataset(),\n        batch_size=config.batch_size,\n        num_workers=config.num_workers,\n        pin_memory=config.pin_memory,\n        persistent_workers=config.persistent_workers,\n        prefetch_factor=config.prefetch_factor\n    )\n    has_started = False\n    for i in range(num_iters):\n        for images, labels in dataloader:\n            if not has_started:  # only start timing after the first iteration\n                has_started = True\n                start_time = time.time()\n            images.to('cuda')\n            labels.to('cuda')\n            torch.cuda.synchronize()\n    end_time = time.time()\n    time_per_epoch = (end_time - start_time) / num_iters\n    print(f'Time taken: {time_per_epoch}')\n</code></pre>"},{"location":"guides/sweep/#full-sweep","title":"\u23f3 Full sweep","text":"<p>A more elaborate sweep can be configured as follows:</p> <pre><code># yaml-language-server: $schema=schemas/data_loader_config.json\nbatch_size: 64\nnum_workers:\n  options:\n  - 1\n  - 2\n  - 4\npersistent_workers: any\npin_memory: any\nprefetch_factor:\n  options:\n  - 1\n  - 2\n  - 4\n</code></pre> <p>We'll keep the batch size fixed at 64 and sweep over the other parameters.</p> <pre><code>python sweep_dataloader.py \\\n  --config_paths \\\n  configs/base/default.yaml \\\n  --sweep_paths \\\n  configs/sweep/full.yaml \\\n  --num_trials 20\n</code></pre> <p>After running this for a while, open up Optuna dashboard to see the results:</p> <pre><code>optuna-dashboard sqlite:///sweep_dataloader.db\n</code></pre> <p>It will show you a web interface to inspect the results. Here's what it looks like:</p> <p></p> <p>You can then quickly see the optimal set of parameters in the bottom left corner.</p>"},{"location":"guides/sweep/#sweep-type-reference","title":"\ud83e\udd38\u200d\u2642\ufe0f Sweep type reference","text":"<p>Below, we give an overview of how field types map to the different options for sweep sampling strategies.</p>"},{"location":"guides/sweep/#built-in-types","title":"\ud83c\udfd7\ufe0f Built-in types","text":""},{"location":"guides/sweep/#1-pgmlintsweep","title":"1\ufe0f\u20e3 <code>pgml.IntSweep</code>","text":"<p>For an <code>int</code> field, <code>pgml.IntSweep</code> will offer the following sampling strategies:</p> <ul> <li>Uniform sampling: <code>low</code>, <code>high</code> and optionally <code>step</code> must be set.</li> <li>Discrete sampling: <code>options</code> must be set.</li> <li>Fixed: just provide an int</li> </ul> <p>Imagine we have the following schema:</p> <pre><code>message Example {\n    int32 int_field = 1;\n}\n</code></pre> <p>For example, this allows you to create the following YAML structures:</p> Uniform<pre><code>int_field:\n  low: 1\n  high: 10\n  step: 1\n</code></pre> Discrete<pre><code>int_field:\n  options:\n  - 1\n  - 2\n  - 3\n</code></pre> Constant<pre><code>int_field: 5\n</code></pre>"},{"location":"guides/sweep/#pgmlfloatsweep","title":"\u26f5 <code>pgml.FloatSweep</code>","text":"<p>For a <code>float</code> field, <code>pgml.FloatSweep</code> will offer the following sampling strategies:</p> <ul> <li>Uniform sampling: <code>low</code>, <code>high</code> and optionally <code>step</code> must be set.</li> <li>Log uniform sampling: <code>log_low</code>, <code>log_high</code> must be set.</li> <li>Discrete sampling: <code>options</code> must be set.</li> <li>Fixed: just provide a float</li> </ul> <p>Imagine we have the following schema:</p> <pre><code>message Example {\n    float float_field = 1;\n}\n</code></pre> <p>This allows you to create the following YAML structures:</p> Uniform<pre><code>float_field:\n  low: 1.0\n  high: 10.0\n  step: 1.0\n</code></pre> Log uniform<pre><code>float_field:\n  log_low: 1.0\n  log_high: 10.0\n</code></pre> Discrete uniform<pre><code>float_field:\n  options:\n  - 1.0\n  - 2.0\n  - 5.0\n</code></pre> Constant<pre><code>float_field: 5.0\n</code></pre>"},{"location":"guides/sweep/#pgmlboolsweep","title":"\u2705 <code>pgml.BoolSweep</code>","text":"<p>For a <code>bool</code> field, <code>pgml.BoolSweep</code> will offer the following sampling strategies:</p> <ul> <li>Any: use a string <code>\"any\"</code>. This samples from both <code>True</code> and <code>False</code>.</li> <li>Fixed: just provide a bool</li> </ul> <p>Imagine we have the following schema:</p> <pre><code>message Example {\n    bool bool_field = 1;\n}\n</code></pre> <p>This allows us to create any of the following YAML files:</p> Uniform<pre><code>bool_field: any\n</code></pre> Constant<pre><code>bool_field: true\n</code></pre>"},{"location":"guides/sweep/#pgmlstringsweep","title":"\u26d3\ufe0f <code>pgml.StringSweep</code>","text":"<p>For a <code>string</code> field, <code>pgml.StringSweep</code> will offer the following sampling strategies:</p> <ul> <li>Choice: provide a list of strings to choose from</li> <li>Fixed: just provide a string</li> </ul> <p>Imagine we have the following schema:</p> <pre><code>message Example {\n    string string_field = 1;\n}\n</code></pre> <p>This allows us to create any of the following YAML files:</p> Uniform<pre><code>string_field:\n  options:\n  - hello\n  - world\n</code></pre> Constant<pre><code>string_field: hello\n</code></pre>"},{"location":"guides/sweep/#custom-types","title":"\ud83e\uddf1 Custom types","text":""},{"location":"guides/sweep/#nested-configs","title":"\ud83d\udc23 Nested configs","text":"<p>With nested configs, the ways to sweep are slightly different. Let's say we have the following schema:</p> <pre><code>message Config {\n    int32 int_field = 1;\n}\n\nmessage Example {\n    Config config_field = 1;\n}\n</code></pre> <p>For the <code>config_field</code> we have the following strategies:</p> <ul> <li>Sweep: just provide one sweep for the <code>config_field</code></li> <li>Nested sweep: provide several sweeps for the <code>config_field</code></li> </ul> <p>This allows us to create any of the following YAML files:</p> Sweep<pre><code>config_field:\n  int_field:\n    low: 1\n    high: 10\n    step: 1\n</code></pre> Nested sweep<pre><code>config_field:\n  nested_options:\n    first:\n      int_field:\n        low: 1\n        high: 10\n        step: 1\n    second:\n      int_field:\n        options:\n        - 1\n        - 2\n        - 3\n</code></pre> <p>For the nested sweep, we'll sample categorically between <code>first</code> and <code>second</code>. We then sample uniformly between 1 and 10 for the <code>int_field</code> in case of <code>first</code> and choose from 1, 2 or 3 for the <code>int_field</code> in case of <code>second</code>.</p>"},{"location":"guides/sweep/#enums","title":"\ud83d\udd20 Enums","text":"<p>For an <code>enum</code> field, <code>py-gen-ml</code> generates a type that enables the following sampling strategies:</p> <ul> <li>Choice: provide a list of enums to choose </li> <li>Any: use a string <code>\"any\"</code>. This samples from all the enum values.</li> <li>Fixed: just provide an enum</li> </ul> <p>Imagine we have the following schema:</p> <pre><code>enum Color {\n    RED = 0;\n    GREEN = 1;\n    BLUE = 2;\n}\n\nmessage Example {\n    Color color_field = 1;\n}\n</code></pre> <p>This allows us to create any of the following YAML files:</p> Discrete with options<pre><code>color_field:\n  options:\n  - RED\n  - GREEN\n  - BLUE\n</code></pre> Discrete with all options<pre><code>color_field: any\n</code></pre> Fixed<pre><code>color_field: RED\n</code></pre>"},{"location":"reference/command_from_func/","title":"pgml_cmd","text":""},{"location":"reference/command_from_func/#pgml_cmd","title":"<code>pgml_cmd</code>","text":"<p>Decorator to create a Typer command from a function.</p> <p>This decorator creates a Typer command from a given function. It expects at least one parameter that is a Pydantic model. This model is used to parse command-line arguments.</p> <p>Other parameters are passed to the function as-is.</p> <p>Parameters:</p> <ul> <li> <code>app</code>               (<code>Typer</code>)           \u2013            <p>The Typer app to add the command to.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name of the command.</p> </li> <li> <code>cls</code>               (<code>Optional[type[TyperCommand]]</code>, default:                   <code>None</code> )           \u2013            <p>The Typer command class to use.</p> </li> <li> <code>context_settings</code>               (<code>Optional[Dict[Any, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Context settings for the command.</p> </li> <li> <code>help</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The help text for the command.</p> </li> <li> <code>epilog</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The epilog text for the command.</p> </li> <li> <code>short_help</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The short help text for the command.</p> </li> <li> <code>options_metavar</code>               (<code>str</code>, default:                   <code>'[OPTIONS]'</code> )           \u2013            <p>The metavar for the options.</p> </li> <li> <code>add_help_option</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add a help option to the command.</p> </li> <li> <code>no_args_is_help</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>When no arguments are provided, whether to show help.</p> </li> <li> <code>hidden</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to hide the command from help messages.</p> </li> <li> <code>deprecated</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to mark the command as deprecated.</p> </li> <li> <code>rich_help_panel</code>               (<code>Union[str, None]</code>, default:                   <code>Default(None)</code> )           \u2013            <p>The rich help panel for the command.</p> </li> </ul>"},{"location":"reference/optuna_sampler/","title":"OptunaSampler","text":""},{"location":"reference/optuna_sampler/#optunasampler","title":"<code>OptunaSampler</code>","text":"<p>               Bases: <code>SweepSampler[TBaseModel]</code></p> <p>A sampler that uses Optuna for hyperparameter optimization.</p> <p>Parameters:</p> <ul> <li> <code>trial</code>               (<code>Trial</code>)           \u2013            <p>The optuna trial to use for hyperparameter optimization.</p> </li> </ul>"},{"location":"reference/optuna_sampler/#py_gen_ml.OptunaSampler.__init__","title":"__init__","text":"<pre><code>__init__(trial: Trial) -&gt; None\n</code></pre> <p>Initialize the OptunaSampler.</p> <p>Parameters:</p> <ul> <li> <code>trial</code>               (<code>Trial</code>)           \u2013            <p>The optuna trial to use for hyperparameter optimization.</p> </li> </ul>"},{"location":"reference/optuna_sampler/#py_gen_ml.OptunaSampler.sample","title":"sample","text":"<pre><code>sample(sweep: Sweeper[TBaseModel]) -&gt; TBaseModel\n</code></pre> <p>Sample a model from the sweep.</p> <p>Parameters:</p> <ul> <li> <code>sweep</code>               (<code>TSweep</code>)           \u2013            <p>The sweep to sample from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TBaseModel</code> (              <code>TBaseModel</code> )          \u2013            <p>The sampled model.</p> </li> </ul>"},{"location":"reference/yaml/","title":"YamlBaseModel","text":""},{"location":"reference/yaml/#yamlbasemodel","title":"<code>YamlBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A base model that can be used to create instances of models from yaml files.</p> <p>This class augments the Pydantic base model with the ability to load data from yaml files. Moreover, it offers the ability to merge multiple yaml files into a single model. It also internally resolves references to other objects in the yaml files. Lastly, it offers the ability to apply cli arguments to the model.</p> <p>Info</p> <p>All Pydantic models that are generated using <code>py-gen-ml</code> will inherit from this class. So make sure to read the documentation on this class to fully leverage all the features of <code>py-gen-ml</code>.</p>"},{"location":"reference/yaml/#py_gen_ml.YamlBaseModel.from_yaml_file","title":"from_yaml_file  <code>classmethod</code>","text":"<pre><code>from_yaml_file(path: str) -&gt; Self\n</code></pre> <p>Create a new instance of the model from a yaml file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the yaml file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>The new instance of the model.</p> </li> </ul>"},{"location":"reference/yaml/#py_gen_ml.YamlBaseModel.load_data_with_references","title":"load_data_with_references  <code>classmethod</code>","text":"<pre><code>load_data_with_references(path: str) -&gt; Dict[str, Any]\n</code></pre> <p>Load data from a yaml file and resolve references.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the yaml file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict[str, Any]: The loaded data.</p> </li> </ul>"},{"location":"reference/yaml/#py_gen_ml.YamlBaseModel.from_yaml_files","title":"from_yaml_files  <code>classmethod</code>","text":"<pre><code>from_yaml_files(paths: List[str]) -&gt; Self\n</code></pre> <p>Create a new instance of the model by merging the data from multiple yaml files.</p> <p>The data from the files must be either the base model type that follows the schema defined in the protobufs, or it must be an overlay that can be merged with the same base model type.</p> <p>Parameters:</p> <ul> <li> <code>paths</code>               (<code>List[str]</code>)           \u2013            <p>The paths to the yaml files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>The new instance of the model.</p> </li> </ul>"},{"location":"reference/yaml/#py_gen_ml.YamlBaseModel.merge","title":"merge","text":"<pre><code>merge(other: BaseModel) -&gt; Self\n</code></pre> <p>Merge this model with another model.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>BaseModel</code>)           \u2013            <p>The other model to merge with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>The merged model.</p> </li> </ul>"},{"location":"reference/yaml/#py_gen_ml.YamlBaseModel.merge_json","title":"merge_json","text":"<pre><code>merge_json(other: Dict[str, Any]) -&gt; Self\n</code></pre> <p>Merges a json representation.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>The other model to merge with as a jsonified dict</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>The merged model.</p> </li> </ul>"},{"location":"reference/yaml/#py_gen_ml.YamlBaseModel.apply_cli_args","title":"apply_cli_args","text":"<pre><code>apply_cli_args(other: BaseModel) -&gt; Self\n</code></pre> <p>Merge CLI args base model.</p> <p>Parameters:</p> <ul> <li> <code>other</code>               (<code>BaseModel</code>)           \u2013            <p>The other model to merge with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>The merged model.</p> </li> </ul>"},{"location":"snippets/src/cli-demo-docs/","title":"command-fn","text":""},{"location":"snippets/src/cli-demo-docs/#command-fn","title":"<code>command-fn</code>","text":"<p>Usage:</p> <pre><code>$ command-fn [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--config-paths TEXT</code>: Paths to config files  [required]</li> <li><code>--sweep-paths TEXT</code>: Paths to sweep files  [default: ]</li> <li><code>--path TEXT</code>: Path to the dataset. Maps to 'data.dataset.path'</li> <li><code>--num-layers INTEGER</code>: Number of layers. Maps to 'model.num_layers'</li> <li><code>--num-epochs INTEGER</code>: Number of epochs. Maps to 'training.num_epochs'</li> <li><code>--num-workers INTEGER</code>: Number of workers for loading the dataset. Maps to 'data.num_workers'</li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"}]}